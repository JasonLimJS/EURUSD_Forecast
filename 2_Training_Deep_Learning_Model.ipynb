{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.Training Deep Learning Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonLimJS/EURUSD_Forecast/blob/master/2_Training_Deep_Learning_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL4YUJsm6Evg",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction:**\n",
        "\n",
        "This document details all the steps involved in training a neural network ensemble. After some hyperparameter tuning, it has been observed that EUR/USD exchange rate could be most accurately being predicted with an ensemble of 10 neural networks where each neural network contains 2  hidden layers that come with sigmoid activation function whereas the output layer is activated via linear function. \n",
        "\n",
        "\n",
        "EUR/USD exchange rate would be most accurately predicted with only USD 3M LIBOR and EUR 3M LIBOR as inputs. The reasons for that might be due to the usage of these 2 rates in a lot of derivatives fixing.\n",
        "\n",
        "**Model performance:**\n",
        "\n",
        "RMSE: 0.13987 (Around 88 pips difference with the actual exchange rate on average)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrgcB4RbYG1k",
        "colab_type": "text"
      },
      "source": [
        "1. Creating normalized daily returns of EURUSD, and select only features of USD 3M LIBOR and EUR 3M LIBOR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ7O4sutYDLp",
        "colab_type": "code",
        "outputId": "61b63ffd-ca48-479d-b391-6f3baa5b93bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.utils import plot_model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from scipy.stats import norm\n",
        "from tensorflow.math import erf\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.simplefilter('ignore')\n",
        "drive.mount('drive')\n",
        "\n",
        "raw_data= pd.read_csv('drive/My Drive/Trading EURUSD with Deep Learning Algorithms/input_data.csv')\n",
        "raw_data['EURUSD_lead']= raw_data['EURUSD'].shift(-1)\n",
        "raw_data['EURUSD_return']= (raw_data.EURUSD_lead/raw_data.EURUSD-1)*100\n",
        "mean_return= np.mean(raw_data['EURUSD_return'])\n",
        "sd_return= np.var(raw_data['EURUSD_return'])**0.5\n",
        "raw_data['EURUSD_return']= (raw_data['EURUSD_return']- np.mean(raw_data['EURUSD_return']))/np.var(raw_data['EURUSD_return'])**0.5\n",
        "raw_data.dropna(inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "raw_data.drop(['EURUSD','EURUSD_lead','vix','bond_spread','ice_swap_2y','ice_swap_3y','ice_swap_5y','ice_swap_7y','snp500','usd_libor_1m','eur_libor_1m'],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "print(raw_data.info())\n",
        "print(raw_data.head(5))\n",
        "print('Mean Return of EURUSD: ' + str(mean_return))\n",
        "print('Standard Deviation of Return of EURUSD: ' + str(sd_return))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 239 entries, 0 to 238\n",
            "Data columns (total 3 columns):\n",
            "usd_libor_3m     239 non-null float64\n",
            "eur_libor_3m     239 non-null float64\n",
            "EURUSD_return    239 non-null float64\n",
            "dtypes: float64(3)\n",
            "memory usage: 7.5 KB\n",
            "None\n",
            "   usd_libor_3m  eur_libor_3m  EURUSD_return\n",
            "0       2.34750      -0.35771       1.400090\n",
            "1       2.34706      -0.35771       1.231940\n",
            "2       2.34156      -0.35557      -0.883293\n",
            "3       2.33531      -0.35614      -1.783914\n",
            "4       2.33488      -0.35643      -0.637832\n",
            "Mean Return of EURUSD: -0.012656776718944591\n",
            "Standard Deviation of Return of EURUSD: 0.37153574053203325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owtm6TXDa18_",
        "colab_type": "text"
      },
      "source": [
        "2. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvQWGGpAPrih",
        "colab_type": "code",
        "outputId": "39d10e50-6d98-43eb-fddb-b3039e73a42a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "raw_data.hist()\n",
        "\n",
        "target= raw_data['EURUSD_return'].values\n",
        "predictors= raw_data.drop(['EURUSD_return'],axis=1).values\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHvpJREFUeJzt3XuYXVWZ5/Hvj4CKhFsIFCEJFAJe\n0kRkLJUZ6OlCvHBRQIZhREQi2OluB4UmjsRoz6BgG20B8YYdpSUqdkAugog2iJR3aROMhBBRiOES\nQgJCIIkgFrzzx14FJ5VTVbtOneuq3+d5zlP7vt99atVba6+99t6KCMzMrPNt1eoAzMysPpzQzcwy\n4YRuZpYJJ3Qzs0w4oZuZZcIJ3cwsE07oZmaZcEI3s7qRtErSG9LwPElfScPdkkLS1q2NMG/ZJvRU\nsJ6UtLHi83lJ50j6RpXlQ9K+abhP0lNpnUckXS1pSsWyl0o6b9D6mxVYSYdI+rmkxyU9Kulnkl6T\n5s2S9ExFXH+Q9FVJL23Qd9En6T2N2LbZUCLinyOiqeVO0gxJiyU9lj4/kDSjmTG0UrYJPXlrREys\n+Jw+inVPj4iJwL7ARODTZVeUtANwPfA5YBIwFfgo8OeKxX6Rtr8j8AbgSWCJpP1HESPNqPG4VmUD\n2qEsjBDDg8DxFH93k4HrgEXNiKsd5J7Qxywi1gPfBl41itVemtb994h4JiKejIgbI+L2Ktt/JiLu\niYj3Aj8CzhluwxVnAqdJug/4YZp+UDojWC/pN5J60/SPA38NfL7iLGWL09/KWnw6g/iZpAsl/RE4\nJ037qaRPp5rPHyQdMYrvxFpM0h6SrpL0cPr9vT9N3+yMU1KvpAcqxldJOlvS7cCmskl9iLPhUyU9\nKGmNpA9ULPtCSZ9J8x5Mwy+sjCfF8BDw1aH2GRHrI2JVFM80EfAMRaVsYD+XSvqipO+lv4efSdo9\n7e8xSb+VdGCZ42tHTugjkLQLcBxw9yhW+x3wjKSFko6QtHPJ9a6mSL5l/A3wCuDNkqYC3wXOo6iZ\nfAC4StKuEfFh4CekM45RnKW8DlgJdAEfr5h2F0XN51PAJZJUcnvWQpK2Ar4D/IbijPEw4ExJby65\niROBo4CdIqJ/DKEcCuwHvAk4W6m9HfgwcBBFxekA4LXARyrW252ibO8FzB5pJ5LWA09RnCX/86DZ\nJ6RtT6Y4a/4FcFsavxK4oIbjagu5J/RvpxrrwOdvR7HuZyU9DjxC8Yt+X9kVI+IJ4BAggC8DD0u6\nTlLXCKs+SFFoyzgnIjZFxJPAO4EbIuKGiHg2Im4CFgNHlo25WiwR8bmI6E/7ALg3Ir4cEc8AC4Ep\nFAnf2t9rgF0j4mMR8XRErKQom28vuf5nI+L+irJQq4+mcruMoqZ9Ypp+EvCxiFgXEQ9TNFGeXLHe\ns8D/i4g/l4khInaiaM48Hfj1oNnXRMSSiHgKuAZ4KiK+lsr15YBr6G3q2IjYqeLzZaAf2KZyIUkD\n43+pmPz+iNgReCWwMzCtYt4W20jjz6YPEbEiImZFxDRgf2AP4DMjxDsVeLTksd1fMbwX8D8r/3lR\n/EOZUn3VUW9/wEMDAxHxpzQ4cQz7sObZC9hjUBmZR/l/yNXKQy0qt3Mvxd8F6ee9Q8wDeDgl4NIi\nYhPwJeBrknarmLW2YvjJKuMdW6ZzT+jV3Ad0D5q2N0WSXj144VSTOA/4QkXzwlDbuD8inq2yjd8C\nl1Ik9uG8jaJ5pIzK5x7fD3x90D+v7SJifpVlATalny+umLb7MNu3znc/8IdBZWT7iDiSojwMVxag\nfuVhesXwnhRnpaSfew0xbyz734ri2KbWuH5HGY8J/fvAyyWdLGkbSZMo2tiuGqZtcCFFTeboNH4V\ncJSkN0maIGkPija5RQCSXi5pjqRpaXw6xanlLwdvOK2/t6TPAb0Up5qj9Q3grZLenLb3onQhaeCs\nYi3wkoGF0yntauCdaflTgX1q2K91jv8ENqQLi9um3/v+KrrSLgWOlDRJ0u7AmQ2M458kvVjSXwHv\npmjiAPh34COSdpU0Gfi/FOV6VCS9UdKB6fh2oGgPfwxYUaf421ruCf072rwf+jURsQ44Avg7YB1w\nB7Ae+IehNhIRTwMXAf+UxpdTJOhPUDSR/AK4leeT8QaKC4i3StpEkcjvAOZUbPa/StoIPAH0ATsA\nr0lnBKMSEfcDx1CcQj9MURv7Pzz/+70IOD5dxf9smva3aZk/An8F/Hy0+7XOkdqH30Jx0fEPFNeG\nvkLRzvx1ioulq4AbeT7JNsKPKDoY3Ax8OiJuTNPPo7juczuwjOIi5XlVtzC8nSj+OTwO3ENRUTl8\ntM01nUp+Y5GZWR5yr6GbmY0bTuhtSNJJg5qKBj7LWx2bjW+S9hyibG6UtGeTYpg3xP6/14z9tzM3\nuZiZZaKpz2WYPHlydHd3N3OXm9m0aRPbbbddy/Zfq06Mu1ExL1my5JGI2LXuG26Qepb5TioHnRJr\np8RZttw3NaF3d3ezePHiZu5yM319ffT29rZs/7XqxLgbFbOke0deqn3Us8x3UjnolFg7Jc6y5d5t\n6GZmmXBCNzPLhBO6mVkmWv6wettc99zvbjFtzsx+ZlWZPmDV/KMaGZJZaQPld6QyW8nlt36c0M0G\nkfQyNr/9/SUUzxbZieKRCQ+n6fMi4oYmh2c2JCd0s0Ei4i7SG6okTaB4kNk1FA+TujAiSr+O0KyZ\n3IZuNrzDgHsioqO6S9r45Bq62fDeTvH0vgGnS3oXxZMB50TEY4NXkDSb9Jq0rq4u+vr66hLIxo0b\n67atRpkzs3gCdde2zw+PpJXH1Anf6WiUfdnrThSP2tyf4kHzp1K8W/Jyihc9rAJOqFa4zTqVpBdQ\nPAP/Q2nSxcC5FH8D5wLnU/wtbCYiFgALAHp6eqJeN650wk0wsyouip6/rFx9cdVJvQ2MaHid8J2O\nRtkml4uA70fEyyle4LoCmAvcHBH7UTzbeG5jQjRrmSOA2yJiLUBErI2IZ9Jbqb5M8SJjs7YxYkKX\ntCPw34FLoHjZQ0Ssp3ihwsK02ELg2EYFadYiJ1LR3CKp8h2tb6N4aYlZ2yhzTrQ3RTetr0o6AFgC\nnAF0RcSatMxDDPGy2Ua1J9aiE9rLqrU7jtQe2Y7H1Anf9XAkbQe8keLNVgM+JelVFE0uqwbNM2u5\nMgl9a+C/AO+LiFslXcSg5pWICElVn8PbqPbEWnRCe1m1mzFGao9sZRvkUDrhux5OemP8LoOmndyi\ncMxKKdOG/gDwQETcmsavpEjwawdOQdPPdY0J0czMyhgxoUfEQ8D96e45KPrl3glcB5ySpp0CXNuQ\nCM3MrJSy/dDfB1yWunGtpLhjbivgCkmnAfcCJzQmRDMzK6NUQo+IpUBPlVmH1TccMzOrlW/9NzPL\nhBO6mVkmnNDNzDLhhG5mlgkndDOzTDihm5llwgndzCwTTuhmZplwQjczy4QTuplZJpzQzcwy4YRu\nZpYJJ3Qzs0w4oZuZZcIJ3cwsE07oZmaZKPvGIrNxRdIqYAPwDNAfET2SJgGXA93AKuCEiHisVTGa\nDeaE3iDdc7/b6hBs7A6NiEcqxucCN0fEfElz0/jZrQnNbEulm1wkTZD0a0nXp/G9Jd0q6W5Jl6f3\njZrl7BhgYRpeCBzbwljMtjCaGvoZwApghzT+SeDCiFgk6UvAacDFdY7PrFUCuFFSAP8aEQuArohY\nk+Y/BHRVW1HSbGA2QFdXF319fXUJaOPGjXXbVqPMmdkPQNe2zw+PpJXH1Anf6WiUSuiSpgFHAR8H\nzpIk4PXAO9IiC4FzcEK3fBwSEasl7QbcJOm3lTMjIlKy30JK/gsAenp6ore3ty4B9fX1Ua9tNcqs\n1NQ4Z2Y/5y8rV19cdVJvAyMaXid8p6NRtob+GeCDwPZpfBdgfUQM/At+AJhabcVG1VZq0cz/xmVr\nJ2WMVNtpxxpGp9d8ImJ1+rlO0jXAa4G1kqZExBpJU4B1LQ3SbJARE7qktwDrImKJpN7R7qBRtZVa\nNPO/8aw6XhQdqbbTyhrOUDq55iNpO2CriNiQht8EfAy4DjgFmJ9+Xtu6KM22VKaGfjBwtKQjgRdR\ntKFfBOwkaetUS58GrG5cmGZN1QVcU7QssjXwzYj4vqRfAVdIOg24FzihhTGabWHEhB4RHwI+BJBq\n6B+IiJMkfQs4HlhE5rWVdu+CWEt8q+Yf1YBI8hARK4EDqkz/I3BY8yMyK2cs/dDPBhZJOg/4NXBJ\nfUIys/HEFZL6GVVCj4g+oC8Nr6S4UGRmZm3Az3IxM8uEE7qZWSac0M3MMuGEbmaWCSd0M7NMOKGb\nmWXCCd3MLBNO6GZmmXBCNzPLhBO6mVkmnNDNzDLhhG5mlomxPG3RzKwl/ITG6lxDNzPLhBO6mVkm\nnNDNzDLhhG5mlgkndLNBJE2XdIukOyUtl3RGmn6OpNWSlqbPka2O1azSiL1cJE0HvkbxJvQAFkTE\nRZImAZcD3cAq4ISIeKxxoZo1TT8wJyJuk7Q9sETSTWnehRHx6RbGZjakMt0Whyrcs4CbI2K+pLnA\nXIoXR1sHcLevoUXEGmBNGt4gaQUwtbVRWat00t/KiAl9mMJ9DNCbFltI8fJoJ3TLiqRu4EDgVuBg\n4HRJ7wIWU1R0tjgrlTQbmA3Q1dVFX19fXWLZuHFj3bbVKHNm9gPQte3zw+2i2ndX5jut5Tha9Xsa\n1Y1Fgwp3V0r2AA9RNMmYZUPSROAq4MyIeELSxcC5FE2P5wLnA6cOXi8iFgALAHp6eqK3t7cu8fT1\n9VGvbTXKrFSbnTOzn/OXtdd9i6tO6t1iWpnvdFYtNfQq+2qG0t94lcL93LyICEkxxHoNqa3UotYa\nTqtrGu1S2xnNd9cJtcnhSNqGorxfFhFXA0TE2or5Xwaub1F4ZlWVSujVCjewVtKUiFgjaQqwrtq6\njaqt1KLWGk4t/6HrqV1qO6OpdXRCbXIoKmorlwArIuKCiulTKs5K3wbc0Yr4zIZSppdL1cINXAec\nAsxPP69tSIRmzXcwcDKwTNLSNG0ecKKkV1E0uawC/q414ZlVV6baN1Thng9cIek04F7ghMaEaNZc\nEfFTQFVm3dDsWMxGo0wvl6EKN8Bh9Q3HzNpBLV31rPVa3zBrZtYE1f5JzZnZ3/JrZPXkW//NzDLh\nhG5mlgk3uVhpo2lXrTyVHS+PDDBrNdfQzcwy4YRuZpYJJ3Qzs0w4oZuZZcIJ3cwsE07oZmaZGHfd\nFn1Ls5nlyjV0M7NMOKGbmWXCCd3MLBNO6GZmmRh3F0XNxht3BBg/nNCt4WpJKH6gl3WyVpV5J3Sz\nDuLatg2noxP6aAv3nJn9dPghW0Zcfq3exlQ6JB0OXARMAL4SEfNr2Y5rHdYp6lXmzRqh5l4ukiYA\nXwCOAGYAJ0qaUa/AzNqNy7y1u7F0W3wtcHdErIyIp4FFwDH1CcusLbnMW1tTRNS2onQ8cHhEvCeN\nnwy8LiJOH7TcbGB2Gn0ZcFft4Y7ZZOCRFu6/Vp0Yd6Ni3isidm3AdkfUBmW+k8pBp8TaKXGWKvcN\nv8ISEQuABY3eTxmSFkdET6vjGK1OjLsTY66XRpX5TvpOOyXWTomzrLE0uawGpleMT0vTzHLlMm9t\nbSwJ/VfAfpL2lvQC4O3AdfUJy6wtucxbW6u5ySUi+iWdDvwHRReuf4uI5XWLrDHaoumnBp0YdyfG\nPKw2KPOd9J12SqydEmcpNV8UNTOz9uKnLZqZZcIJ3cwsE+MuoUv6F0m/lXS7pGsk7dTqmIYi6XBJ\nd0m6W9LcVsczEknTJd0i6U5JyyWd0eqYOo2kSZJukvT79HPnKsvsJek2SUvT9/z3FfNOlLQsle/v\nS5rcpnG+QNICSb9Lf4//oxFx1iPWimWuk3RHo+Ksh3HXhi7pTcAP0wWuTwJExNktDmsL6Tbz3wFv\nBB6g6GFxYkTc2dLAhiFpCjAlIm6TtD2wBDi2nWNuN5I+BTwaEfPTP/GdB5fP1MNGEfFnSROBO4D/\nBqwDHgRmRMQjaVt/iohz2inOiHhQ0keBCRHxEUlbAZMioiE3+Iw11jT/OOB44JURsX8j4qyHcVdD\nj4gbI6I/jf6Soi9xO+q428wjYk1E3JaGNwArgKmtjarjHAMsTMMLgWMHLxART0fEn9PoC3n+71jp\ns50kATtQJPh2ixPgVOATablnG5XM6xFrSvBnAec1MMa6GHcJfZBTge+1OoghTAXurxh/gA5KjpK6\ngQOBW1sbScfpiog1afghoKvaQql563aKMvLJiHgwIv4C/AOwjFRTBy5ptzgrmjnPTc0c35JUdf1W\nxzoQJ3A+8KcGxlgXWT5cWdIPgN2rzPpwRFyblvkw0A9c1szYxoNUo7kKODMinmh1PO1muPJZORIR\nIalqm2hE3A+8UtIewLclXQk8SpHQDwRWAp8DPkSNNcsGxvkMxZnxzyPiLElnAZ8GTq4lzgbHOgXY\nJyL+MVVS2lqWCT0i3jDcfEmzgLcAh0X7XkToyNvMJW1Dkcwvi4irWx1POxqufEpaK2lKRKxJ1yTW\njbCtB9OFur8G7k3T7knbugKo+WJ6A+O8iqK2O1A+vgWcVmucDY51V6BH0iqKfLmbpL6I6B1LvI0y\n7ppcVLyg4IPA0RExplMoSZdKGrb2I6lX0gMV48sl9abhcyR9Y4hVO+4289RuewmwIiIuaHU8Heo6\n4JQ0fApw7eAFJE2TtG0a3hk4hOKJjquBGZIGnsr3RorrGG0VZ6pEfQfoTYseBjTywvlYYr04IvaI\niO407XftmswBiIhx9QHupmgjW5o+XxrDti4FzhthmV7ggSHmnQN8Y5h1j6To6XIPRXNRrXH+I8Up\n+BMUbasXAls34Ls9BAjg9orv98hW/8476QPsAtwM/B74AUXvD4AeijckQZGobwd+k37Orlj/7ymS\n+O0USXOXNo1zL+DHafrNwJ7t+p1WbKcbuKPVZWS4z7jrtlhPki6lSNYfGWaZXoqkvUVvGknnAPtG\nxDvrEMuEiHhmiHn7AH+MiPWSJgFXAteHa9FmWRl3TS4AkkLSvhXjzzWdSJos6XpJ6yU9KuknqZ8s\nkg5MV+U3SLoceFEN+14lqbK970WSLk/bvE3SARXLvkJSX4pluaSjB8V8saQbJG0CDh1qnxFxT0Ss\nH1gVeBaoPP6Q9F4VN15skHSupH0k/VzSE5KuSM0+ZtbGxmVCH8Ecii6Cu1J0b5oHREpo3wa+Dkyi\nuJBTj7vbjknbmgR8k+Lq+jbp4uJ3gBuB3YD3AZdJelnFuu8APg5sD/x0uJ1IeoekJyjeznIA8K+D\nFnkz8GrgIIprDAuAd1JcmN0fOHEMx2hmTeCEvqW/UHRV2isi/hIRP4miXeogYBvgM2n6lRQXLsdq\nSURcGUUf4gsoav0Hpc9EYH4UNz38ELiezRPrtRHxsyhuzHhquJ1ExDcjYgfgpcCXgLWDFvlURDwR\nxeNg7wBujOKmpscp+uofWIdjNbMGckLf0r9QXDi9UdJKPf8MlT2A1bH5RYd767C/524eiohnKc4O\n9kif+9O0yv1NrbZuWRHxe2A58MVBsyoT/JNVxieOdl9m1lzjNaH/CXhxxfhzNyRExIaImBMRLwGO\nBs6SdBiwBpiauuYN2LMOsTzX1zy11U+j6InyIDB9oP2+Yn+VfdFrvaK9NbBPjeuaWZsarwl9KfAO\nSRNSv/S/GZgh6S2S9k2J+3GKu9qeBX5BcWfp+1Mb93EUz1sZq1dLOk7S1sCZwJ8pnjFzK8U/ng+m\n/fUCb6V4psuoSHqPpN3S8AyKuwdvrkPsZtZGxmtCP4MiOa4HTqK42DlgP4q+qhspkvgXI+KWKB6Q\ndRwwi+IW6//F83e6jcW1aVuPUdz6fFxqo386xXgExYXMLwLviojf1rCPg4FlqTfMDekzrw6xm1kb\ncT90M7NMjNcauplZdpzQ60DSPEkbq3ya9mjedONRtRhOalYMZtZabnIxM8tEUx+fO3ny5Oju7m7m\nLgHYtGkT2223XdP32wg5HQuM/niWLFnySETsOvKSZuNPUxN6d3c3ixcvbuYuAejr66O3t7fp+22E\nnI4FRn88kupxM5dZltyGbmaWCSd0M7NMOKGbmWWiLd4p2j33uzWtt2r+UXWOxMysc7mGbmaWCSd0\nM7NMtEWTiz1vpOanOTP7mTVoGTc9mRm4hm5mlg0ndDOzTDihm5llwgndzCwTTuhmZplwQjczy4QT\nuplZJpzQzcwy4YRuZpYJJ3Qzs0w4oZuZZcIJ3cwsE6UTuqQJkn4t6fo0vrekWyXdLelySS9oXJhm\nZjaS0dTQzwBWVIx/ErgwIvYFHgNOq2dgZmY2OqUSuqRpwFHAV9K4gNcDV6ZFFgLHNiJAMzMrRxEx\n8kLSlcAngO2BDwCzgF+m2jmSpgPfi4j9q6w7G5gN0NXV9epFixZtsf1lqx+vKfiZU3cstdzGjRuZ\nOHFiTftotpG+i65tYe2Tm08r+z20o9H+bg499NAlEdHTwJDMOtaIL7iQ9BZgXUQskdQ72h1ExAJg\nAUBPT0/09m65icEvbChr1Unlwunr66PaftvRSN/FnJn9nL9s819b2e+hHXXS78as3ZV5Y9HBwNGS\njgReBOwAXATsJGnriOgHpgGrGxemmZmNZMQ29Ij4UERMi4hu4O3ADyPiJOAW4Pi02CnAtQ2L0szM\nRjSWfuhnA2dJuhvYBbikPiGZmVktRvWS6IjoA/rS8ErgtfUPyczMauE7Rc3MMuGEbmaWCSd0M7NM\nOKGbmWXCCd3MLBNO6GZmmXBCNzPLhBO6mVkmnNDNzDLhhG5mlgkndDOzTDihm5llwgndzCwTTuhm\nZplwQjczy8SonoduVovuYd6TOmdmf9X3qK6af1QjQzLLkmvoZmaZcEI3M8uEE7qZWSac0M3MMuGE\nbmaWCSd0M7NMOKGbmWXCCd3MLBO+sahBhruZxsysEVxDNzPLxIgJXdJ0SbdIulPScklnpOmTJN0k\n6ffp586ND9fMzIZSpobeD8yJiBnAQcD/ljQDmAvcHBH7ATencTMza5ERE3pErImI29LwBmAFMBU4\nBliYFlsIHNuoIM3MbGSKiPILS93Aj4H9gfsiYqc0XcBjA+OD1pkNzAbo6up69aJFi7bY7rLVj9cQ\nOsycumOp5TZu3MjEiRNr2ketaj2mkXRtC2uf3Hxa2e+hVYb7LqodDwx9TIceeuiSiOipV2xmOSmd\n0CVNBH4EfDwirpa0vjKBS3osIoZtR+/p6YnFixdvMb3WHiFlH7Ha19dHb29vTfuoVaN6ucyZ2c/5\nyzbvnNTuj5od6fG5g48Hhj4mSU7oZkMo1W1R0jbAVcBlEXF1mrxW0pSIWCNpCrCuUUFae3BXTLP2\nVqaXi4BLgBURcUHFrOuAU9LwKcC19Q/PzMzKKlNDPxg4GVgmaWmaNg+YD1wh6TTgXuCExoRoZmZl\njJjQI+KngIaYfVh9wzEzs1r5TlEzs0w4oZuZZaKjH85VttdF5Zvl272LXy1q6X2S4/dgNt65hm5m\nlgkndDOzTHR0k0st3DxhZrlyDd3MLBPjroZuBd/Gb5Yf19DNzDLhhG5mlgkndDOzTDihm5llwhdF\nS/AFRDPrBK6hm5llwgndzCwTTuhmZplwQjczy4QTuplZJpzQzcwy4YRuZpYJJ3Qzs0w4oZuZZcIJ\n3cwsE07oZmaZcEI3M8uEE7qZWSac0M3MMjGmhC7pcEl3Sbpb0tx6BWVmZqNXc0KXNAH4AnAEMAM4\nUdKMegVmZmajM5Ya+muBuyNiZUQ8DSwCjqlPWGZmNlqKiNpWlI4HDo+I96Txk4HXRcTpg5abDcxO\noy8D7qo93JpNBh5pwX4bIadjgdEfz14RsWujgjHrZA1/BV1ELAAWNHo/w5G0OCJ6WhlDveR0LJDf\n8Zi10liaXFYD0yvGp6VpZmbWAmNJ6L8C9pO0t6QXAG8HrqtPWGZmNlo1N7lERL+k04H/ACYA/xYR\ny+sWWX21tMmnznI6FsjveMxapuaLomZm1l58p6iZWSac0M3MMpFNQpc0XdItku6UtFzSGcMs+xpJ\n/akvfdspeyySeiUtTcv8qNlxllHmWCTtKOk7kn6Tlnl3K2I163TZtKFLmgJMiYjbJG0PLAGOjYg7\nBy03AbgJeIriQu6VzY92eGWORdJOwM8pbu66T9JuEbGuRSEPqeSxzAN2jIizJe1KcfPZ7ukOZDMr\nKZsaekSsiYjb0vAGYAUwtcqi7wOuAtou+Q0oeSzvAK6OiPvScm15PCWPJYDtJQmYCDwK9Dc1ULMM\nZJPQK0nqBg4Ebh00fSrwNuDi5kdVm6GOBXgpsLOkPklLJL2r2bGN1jDH8nngFcCDwDLgjIh4tqnB\nmWWg4bf+N5ukiRQ18DMj4olBsz8DnB0RzxaVwfY2wrFsDbwaOAzYFviFpF9GxO+aHGYpIxzLm4Gl\nwOuBfYCbJP2kynJmNoysErqkbSiSxmURcXWVRXqARSmZTwaOlNQfEd9uYpillDiWB4A/RsQmYJOk\nHwMHAG2X0Escy7uB+VFc0Llb0h+AlwP/2cQwzTpeNk0uqf31EmBFRFxQbZmI2DsiuiOiG7gSeG+b\nJvMRjwW4FjhE0taSXgy8jqJ9uq2UPJb7KM40kNRF8VTOlc2J0CwfOdXQDwZOBpZJWpqmzQP2BIiI\nL7UqsBqMeCwRsULS94HbgWeBr0TEHS2Jdnhlfi/nApdKWgaIolksp0cEmzVFNt0WzczGu2yaXMzM\nxjsndDOzTDihm5llwgndzCwTTuhmZplwQjczy4QTuplZJv4/IOD50PPDaCQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-hCs113bSDF",
        "colab_type": "text"
      },
      "source": [
        "3. Initialize Function to Train Deep Learning Model (2 Hidden Layers with Sigmoid Activation Functions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0l-Vk3ObeVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "act_layer1_ls=[]\n",
        "input_col_ls=[]\n",
        "neurons_layer1_ls=[]\n",
        "ind_layer2_ls=[]\n",
        "act_layer2_ls=[]\n",
        "neurons_layer2_ls=[]\n",
        "ind_output_ls=[]\n",
        "act_output_ls=[]\n",
        "lr_ls=[]\n",
        "bs_ls=[]\n",
        "ep_ls=[]\n",
        "RMSE_ls=[]\n",
        "uniq_count_ls=[]\n",
        "#RMSE_min= np.Inf\n",
        "#best_model=None\n",
        "n_cols= predictors.shape[1]\n",
        "\n",
        "def custom_tanh(x):\n",
        "  return 10*K.tanh(x)\n",
        "\n",
        "def custom_Gaussian(x):\n",
        "  return K.exp(-K.pow(x,2))\n",
        "\n",
        "def custom_soft_clip(x,alpha=1):\n",
        "  return (1/alpha)*K.log((1+K.exp(x*alpha))/(1+K.exp(x*alpha-1)))\n",
        "\n",
        "def ANN_training(act_layer1='relu',input_col=n_cols,neurons_layer1=25,ind_layer2=False,act_layer2=None,neurons_layer2=None,ind_output=False,act_output=None,\n",
        "                 lr=0.0001,bs=10,ep=250,RMSE_min=RMSE_min,best_model=best_model,stop_step=300):\n",
        "  \n",
        "  model= Sequential()\n",
        "  \n",
        "  #Input layer to Hidden Layer 1\n",
        "  model.add(Dense(neurons_layer1,activation=act_layer1,input_shape=(input_col,)))\n",
        "  \n",
        "  if ind_layer2==True:\n",
        "    #Hidden Layer 1 to Hidden Layer 2\n",
        "    model.add(Dense(neurons_layer2,activation=act_layer2))\n",
        "    \n",
        "  if ind_output==True:\n",
        "    #Hidden Layer 2 to Output Layer\n",
        "    model.add(Dense(1,activation=act_output))  \n",
        "  else:\n",
        "    model.add(Dense(1))\n",
        "\n",
        "  adam= Adam(lr=lr)\n",
        "  model.compile(optimizer=adam, loss='mean_squared_error')\n",
        "  \n",
        "  train_index_end= int(len(predictors)*0.8)\n",
        "  test_index_end= len(predictors)\n",
        "  \n",
        "  X_train_pre= predictors[0:train_index_end]\n",
        "  X_test_pre= predictors[train_index_end:test_index_end]\n",
        "  y_train_pre= target[0:train_index_end]\n",
        "  y_test_pre= target[train_index_end:test_index_end]\n",
        "  \n",
        "  X_norm= MinMaxScaler()\n",
        "  Y_norm= MinMaxScaler()\n",
        "  \n",
        "  X_train= X_norm.fit_transform(X_train_pre)\n",
        "  X_test= X_norm.transform(X_test_pre)\n",
        "  y_train= Y_norm.fit_transform(y_train_pre.reshape(-1,1))\n",
        "  y_test= Y_norm.transform(y_test_pre.reshape(-1,1))\n",
        "  \n",
        "  early_stopping_monitor= EarlyStopping(patience=stop_step)\n",
        "  \n",
        "  model.fit(X_train,y_train,epochs=ep,verbose=True,validation_split=0.4,batch_size=bs,callbacks=[early_stopping_monitor])\n",
        "  \n",
        "  y_pred= model.predict(X_test)\n",
        "  RMSE= MSE(y_test,y_pred)**0.5\n",
        "  uniq_count= len(np.unique(y_pred))\n",
        "  \n",
        "  RMSE_ls.append(RMSE)\n",
        "  uniq_count_ls.append(uniq_count)\n",
        "  \n",
        "  #if RMSE < RMSE_min:\n",
        "    #best_model= model\n",
        "    #RMSE_min=RMSE\n",
        "  #else:\n",
        "    #best_model= best_model\n",
        "    #RMSE_min= RMSE_min\n",
        "    \n",
        "    \n",
        "  return model, RMSE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eovEn7LuanYc",
        "colab_type": "text"
      },
      "source": [
        "4. Training 10 Deep Learning Models (2 Hidden Layers with Sigmoid Activation Functions)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db3vnfrqcG77",
        "colab_type": "code",
        "outputId": "b1bfa03e-bc30-40df-80f3-7dc3fa9e7a34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "input_col= n_cols\n",
        "ind_layer2= False\n",
        "act_layer2= None\n",
        "neurons_layer2= None\n",
        "ind_output= False\n",
        "act_output= None\n",
        "\n",
        "        \n",
        "op_ind= 0\n",
        "stop_step= 10\n",
        "\n",
        "for i in range(1,11):\n",
        "  for act_layer1 in ['sigmoid']:\n",
        "    for neurons_layer1 in [25]:\n",
        "      for lr in [0.001]:\n",
        "       for bs in [100]:\n",
        "          for ep in [300]:\n",
        "\n",
        "\n",
        "            act_layer1_ls.append(act_layer1)\n",
        "            input_col_ls.append(input_col)\n",
        "            neurons_layer1_ls.append(neurons_layer1)\n",
        "            ind_layer2_ls.append(ind_layer2)\n",
        "            act_layer2_ls.append(act_layer2)\n",
        "            neurons_layer2_ls.append(neurons_layer2)\n",
        "            ind_output_ls.append(ind_output)\n",
        "            act_output_ls.append(act_output)\n",
        "            lr_ls.append(lr)\n",
        "            bs_ls.append(bs)\n",
        "            ep_ls.append(ep)\n",
        "\n",
        "            locals() ['model_train_' + str(i)], RMSE=ANN_training(act_layer1= act_layer1,\n",
        "                                                      input_col= input_col,\n",
        "                                                      neurons_layer1= neurons_layer1,\n",
        "                                                      ind_layer2= ind_layer2,\n",
        "                                                      act_layer2= act_layer2,\n",
        "                                                      neurons_layer2= neurons_layer2,\n",
        "                                                      ind_output= ind_output,\n",
        "                                                      act_output= act_output,\n",
        "                                                      lr= lr,\n",
        "                                                      bs= bs,\n",
        "                                                      ep= ep,\n",
        "                                                      RMSE_min= RMSE_min,\n",
        "                                                      best_model=best_model,\n",
        "                                                      stop_step= stop_step)\n",
        "            op_ind += 1\n",
        "\n",
        "            print(\"Operation \" + str(op_ind) + \" completed!\")\n",
        "            print('RMSE of the trained model ' + str(i) + ' is: ' + str(RMSE))\n",
        "\n",
        "       \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 114 samples, validate on 77 samples\n",
            "Epoch 1/300\n",
            "114/114 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.3415\n",
            "Epoch 2/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.3762 - val_loss: 0.3055\n",
            "Epoch 3/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.3414 - val_loss: 0.2720\n",
            "Epoch 4/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.3086 - val_loss: 0.2409\n",
            "Epoch 5/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 0.2779 - val_loss: 0.2122\n",
            "Epoch 6/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.2494 - val_loss: 0.1859\n",
            "Epoch 7/300\n",
            "114/114 [==============================] - 0s 78us/step - loss: 0.2230 - val_loss: 0.1619\n",
            "Epoch 8/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 0.1985 - val_loss: 0.1399\n",
            "Epoch 9/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.1767 - val_loss: 0.1202\n",
            "Epoch 10/300\n",
            "114/114 [==============================] - 0s 74us/step - loss: 0.1565 - val_loss: 0.1028\n",
            "Epoch 11/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.1387 - val_loss: 0.0876\n",
            "Epoch 12/300\n",
            "114/114 [==============================] - 0s 98us/step - loss: 0.1223 - val_loss: 0.0745\n",
            "Epoch 13/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.1085 - val_loss: 0.0631\n",
            "Epoch 14/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.0958 - val_loss: 0.0534\n",
            "Epoch 15/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.0854 - val_loss: 0.0452\n",
            "Epoch 16/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.0761 - val_loss: 0.0385\n",
            "Epoch 17/300\n",
            "114/114 [==============================] - 0s 80us/step - loss: 0.0682 - val_loss: 0.0333\n",
            "Epoch 18/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.0618 - val_loss: 0.0292\n",
            "Epoch 19/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 0.0563 - val_loss: 0.0261\n",
            "Epoch 20/300\n",
            "114/114 [==============================] - 0s 104us/step - loss: 0.0524 - val_loss: 0.0238\n",
            "Epoch 21/300\n",
            "114/114 [==============================] - 0s 104us/step - loss: 0.0488 - val_loss: 0.0223\n",
            "Epoch 22/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 0.0462 - val_loss: 0.0214\n",
            "Epoch 23/300\n",
            "114/114 [==============================] - 0s 147us/step - loss: 0.0442 - val_loss: 0.0209\n",
            "Epoch 24/300\n",
            "114/114 [==============================] - 0s 104us/step - loss: 0.0429 - val_loss: 0.0207\n",
            "Epoch 25/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.0417 - val_loss: 0.0208\n",
            "Epoch 26/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 0.0410 - val_loss: 0.0211\n",
            "Epoch 27/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.0405 - val_loss: 0.0214\n",
            "Epoch 28/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 0.0402 - val_loss: 0.0219\n",
            "Epoch 29/300\n",
            "114/114 [==============================] - 0s 72us/step - loss: 0.0401 - val_loss: 0.0223\n",
            "Epoch 30/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.0401 - val_loss: 0.0227\n",
            "Epoch 31/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.0400 - val_loss: 0.0231\n",
            "Epoch 32/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 0.0400 - val_loss: 0.0234\n",
            "Epoch 33/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 0.0400 - val_loss: 0.0237\n",
            "Epoch 34/300\n",
            "114/114 [==============================] - 0s 73us/step - loss: 0.0401 - val_loss: 0.0240\n",
            "Operation 1 completed!\n",
            "RMSE of the trained model 1 is: 0.1506224346615768\n",
            "Train on 114 samples, validate on 77 samples\n",
            "Epoch 1/300\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 1.8699 - val_loss: 1.8596\n",
            "Epoch 2/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 1.7816 - val_loss: 1.7642\n",
            "Epoch 3/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 1.6963 - val_loss: 1.6719\n",
            "Epoch 4/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 1.6131 - val_loss: 1.5827\n",
            "Epoch 5/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 1.5322 - val_loss: 1.4964\n",
            "Epoch 6/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 1.4539 - val_loss: 1.4128\n",
            "Epoch 7/300\n",
            "114/114 [==============================] - 0s 78us/step - loss: 1.3778 - val_loss: 1.3317\n",
            "Epoch 8/300\n",
            "114/114 [==============================] - 0s 75us/step - loss: 1.3049 - val_loss: 1.2536\n",
            "Epoch 9/300\n",
            "114/114 [==============================] - 0s 75us/step - loss: 1.2332 - val_loss: 1.1785\n",
            "Epoch 10/300\n",
            "114/114 [==============================] - 0s 73us/step - loss: 1.1649 - val_loss: 1.1061\n",
            "Epoch 11/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 1.0990 - val_loss: 1.0367\n",
            "Epoch 12/300\n",
            "114/114 [==============================] - 0s 95us/step - loss: 1.0358 - val_loss: 0.9705\n",
            "Epoch 13/300\n",
            "114/114 [==============================] - 0s 70us/step - loss: 0.9750 - val_loss: 0.9075\n",
            "Epoch 14/300\n",
            "114/114 [==============================] - 0s 74us/step - loss: 0.9170 - val_loss: 0.8475\n",
            "Epoch 15/300\n",
            "114/114 [==============================] - 0s 118us/step - loss: 0.8618 - val_loss: 0.7903\n",
            "Epoch 16/300\n",
            "114/114 [==============================] - 0s 98us/step - loss: 0.8090 - val_loss: 0.7359\n",
            "Epoch 17/300\n",
            "114/114 [==============================] - 0s 112us/step - loss: 0.7589 - val_loss: 0.6843\n",
            "Epoch 18/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.7113 - val_loss: 0.6357\n",
            "Epoch 19/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.6659 - val_loss: 0.5899\n",
            "Epoch 20/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.6231 - val_loss: 0.5467\n",
            "Epoch 21/300\n",
            "114/114 [==============================] - 0s 98us/step - loss: 0.5824 - val_loss: 0.5059\n",
            "Epoch 22/300\n",
            "114/114 [==============================] - 0s 101us/step - loss: 0.5442 - val_loss: 0.4673\n",
            "Epoch 23/300\n",
            "114/114 [==============================] - 0s 97us/step - loss: 0.5078 - val_loss: 0.4311\n",
            "Epoch 24/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.4733 - val_loss: 0.3971\n",
            "Epoch 25/300\n",
            "114/114 [==============================] - 0s 74us/step - loss: 0.4410 - val_loss: 0.3651\n",
            "Epoch 26/300\n",
            "114/114 [==============================] - 0s 90us/step - loss: 0.4108 - val_loss: 0.3354\n",
            "Epoch 27/300\n",
            "114/114 [==============================] - 0s 108us/step - loss: 0.3820 - val_loss: 0.3078\n",
            "Epoch 28/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.3551 - val_loss: 0.2819\n",
            "Epoch 29/300\n",
            "114/114 [==============================] - 0s 97us/step - loss: 0.3304 - val_loss: 0.2579\n",
            "Epoch 30/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.3071 - val_loss: 0.2359\n",
            "Epoch 31/300\n",
            "114/114 [==============================] - 0s 90us/step - loss: 0.2848 - val_loss: 0.2154\n",
            "Epoch 32/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.2648 - val_loss: 0.1962\n",
            "Epoch 33/300\n",
            "114/114 [==============================] - 0s 95us/step - loss: 0.2460 - val_loss: 0.1785\n",
            "Epoch 34/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 0.2282 - val_loss: 0.1624\n",
            "Epoch 35/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 0.2118 - val_loss: 0.1475\n",
            "Epoch 36/300\n",
            "114/114 [==============================] - 0s 100us/step - loss: 0.1968 - val_loss: 0.1338\n",
            "Epoch 37/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 0.1828 - val_loss: 0.1214\n",
            "Epoch 38/300\n",
            "114/114 [==============================] - 0s 73us/step - loss: 0.1699 - val_loss: 0.1101\n",
            "Epoch 39/300\n",
            "114/114 [==============================] - 0s 102us/step - loss: 0.1581 - val_loss: 0.0999\n",
            "Epoch 40/300\n",
            "114/114 [==============================] - 0s 98us/step - loss: 0.1471 - val_loss: 0.0906\n",
            "Epoch 41/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 0.1372 - val_loss: 0.0820\n",
            "Epoch 42/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.1280 - val_loss: 0.0743\n",
            "Epoch 43/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.1195 - val_loss: 0.0673\n",
            "Epoch 44/300\n",
            "114/114 [==============================] - 0s 65us/step - loss: 0.1116 - val_loss: 0.0610\n",
            "Epoch 45/300\n",
            "114/114 [==============================] - 0s 119us/step - loss: 0.1046 - val_loss: 0.0554\n",
            "Epoch 46/300\n",
            "114/114 [==============================] - 0s 116us/step - loss: 0.0982 - val_loss: 0.0504\n",
            "Epoch 47/300\n",
            "114/114 [==============================] - 0s 103us/step - loss: 0.0923 - val_loss: 0.0461\n",
            "Epoch 48/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 0.0869 - val_loss: 0.0421\n",
            "Epoch 49/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 0.0821 - val_loss: 0.0386\n",
            "Epoch 50/300\n",
            "114/114 [==============================] - 0s 74us/step - loss: 0.0778 - val_loss: 0.0355\n",
            "Epoch 51/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.0738 - val_loss: 0.0328\n",
            "Epoch 52/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.0702 - val_loss: 0.0305\n",
            "Epoch 53/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.0669 - val_loss: 0.0284\n",
            "Epoch 54/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 0.0641 - val_loss: 0.0266\n",
            "Epoch 55/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.0615 - val_loss: 0.0251\n",
            "Epoch 56/300\n",
            "114/114 [==============================] - 0s 74us/step - loss: 0.0591 - val_loss: 0.0238\n",
            "Epoch 57/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.0570 - val_loss: 0.0228\n",
            "Epoch 58/300\n",
            "114/114 [==============================] - 0s 100us/step - loss: 0.0552 - val_loss: 0.0219\n",
            "Epoch 59/300\n",
            "114/114 [==============================] - 0s 75us/step - loss: 0.0535 - val_loss: 0.0211\n",
            "Epoch 60/300\n",
            "114/114 [==============================] - 0s 71us/step - loss: 0.0520 - val_loss: 0.0205\n",
            "Epoch 61/300\n",
            "114/114 [==============================] - 0s 70us/step - loss: 0.0506 - val_loss: 0.0200\n",
            "Epoch 62/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.0494 - val_loss: 0.0196\n",
            "Epoch 63/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.0483 - val_loss: 0.0193\n",
            "Epoch 64/300\n",
            "114/114 [==============================] - 0s 107us/step - loss: 0.0474 - val_loss: 0.0191\n",
            "Epoch 65/300\n",
            "114/114 [==============================] - 0s 97us/step - loss: 0.0465 - val_loss: 0.0190\n",
            "Epoch 66/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 0.0458 - val_loss: 0.0190\n",
            "Epoch 67/300\n",
            "114/114 [==============================] - 0s 80us/step - loss: 0.0452 - val_loss: 0.0190\n",
            "Epoch 68/300\n",
            "114/114 [==============================] - 0s 75us/step - loss: 0.0447 - val_loss: 0.0190\n",
            "Epoch 69/300\n",
            "114/114 [==============================] - 0s 61us/step - loss: 0.0443 - val_loss: 0.0190\n",
            "Epoch 70/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.0439 - val_loss: 0.0191\n",
            "Epoch 71/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.0435 - val_loss: 0.0192\n",
            "Epoch 72/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.0432 - val_loss: 0.0193\n",
            "Epoch 73/300\n",
            "114/114 [==============================] - 0s 66us/step - loss: 0.0429 - val_loss: 0.0195\n",
            "Epoch 74/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 0.0426 - val_loss: 0.0196\n",
            "Epoch 75/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.0424 - val_loss: 0.0198\n",
            "Epoch 76/300\n",
            "114/114 [==============================] - 0s 61us/step - loss: 0.0422 - val_loss: 0.0200\n",
            "Epoch 77/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.0420 - val_loss: 0.0202\n",
            "Operation 2 completed!\n",
            "RMSE of the trained model 2 is: 0.14448359291723062\n",
            "Train on 114 samples, validate on 77 samples\n",
            "Epoch 1/300\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.4566 - val_loss: 0.3752\n",
            "Epoch 2/300\n",
            "114/114 [==============================] - 0s 104us/step - loss: 0.4170 - val_loss: 0.3367\n",
            "Epoch 3/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.3798 - val_loss: 0.3007\n",
            "Epoch 4/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.3445 - val_loss: 0.2670\n",
            "Epoch 5/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.3117 - val_loss: 0.2359\n",
            "Epoch 6/300\n",
            "114/114 [==============================] - 0s 67us/step - loss: 0.2807 - val_loss: 0.2073\n",
            "Epoch 7/300\n",
            "114/114 [==============================] - 0s 67us/step - loss: 0.2526 - val_loss: 0.1812\n",
            "Epoch 8/300\n",
            "114/114 [==============================] - 0s 69us/step - loss: 0.2259 - val_loss: 0.1575\n",
            "Epoch 9/300\n",
            "114/114 [==============================] - 0s 75us/step - loss: 0.2021 - val_loss: 0.1359\n",
            "Epoch 10/300\n",
            "114/114 [==============================] - 0s 80us/step - loss: 0.1799 - val_loss: 0.1166\n",
            "Epoch 11/300\n",
            "114/114 [==============================] - 0s 90us/step - loss: 0.1600 - val_loss: 0.0993\n",
            "Epoch 12/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.1418 - val_loss: 0.0841\n",
            "Epoch 13/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.1257 - val_loss: 0.0709\n",
            "Epoch 14/300\n",
            "114/114 [==============================] - 0s 75us/step - loss: 0.1111 - val_loss: 0.0596\n",
            "Epoch 15/300\n",
            "114/114 [==============================] - 0s 71us/step - loss: 0.0987 - val_loss: 0.0500\n",
            "Epoch 16/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.0875 - val_loss: 0.0420\n",
            "Epoch 17/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.0784 - val_loss: 0.0354\n",
            "Epoch 18/300\n",
            "114/114 [==============================] - 0s 99us/step - loss: 0.0703 - val_loss: 0.0303\n",
            "Epoch 19/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.0635 - val_loss: 0.0263\n",
            "Epoch 20/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 0.0582 - val_loss: 0.0234\n",
            "Epoch 21/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.0536 - val_loss: 0.0214\n",
            "Epoch 22/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.0501 - val_loss: 0.0201\n",
            "Epoch 23/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.0475 - val_loss: 0.0193\n",
            "Epoch 24/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.0453 - val_loss: 0.0191\n",
            "Epoch 25/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 0.0437 - val_loss: 0.0192\n",
            "Epoch 26/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.0428 - val_loss: 0.0196\n",
            "Epoch 27/300\n",
            "114/114 [==============================] - 0s 90us/step - loss: 0.0420 - val_loss: 0.0201\n",
            "Epoch 28/300\n",
            "114/114 [==============================] - 0s 102us/step - loss: 0.0415 - val_loss: 0.0207\n",
            "Epoch 29/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 0.0413 - val_loss: 0.0212\n",
            "Epoch 30/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.0411 - val_loss: 0.0217\n",
            "Epoch 31/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.0410 - val_loss: 0.0221\n",
            "Epoch 32/300\n",
            "114/114 [==============================] - 0s 65us/step - loss: 0.0410 - val_loss: 0.0225\n",
            "Epoch 33/300\n",
            "114/114 [==============================] - 0s 71us/step - loss: 0.0410 - val_loss: 0.0230\n",
            "Epoch 34/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.0411 - val_loss: 0.0234\n",
            "Operation 3 completed!\n",
            "RMSE of the trained model 3 is: 0.13510406088724272\n",
            "Train on 114 samples, validate on 77 samples\n",
            "Epoch 1/300\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.4186 - val_loss: 0.3549\n",
            "Epoch 2/300\n",
            "114/114 [==============================] - 0s 106us/step - loss: 0.3797 - val_loss: 0.3162\n",
            "Epoch 3/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.3435 - val_loss: 0.2799\n",
            "Epoch 4/300\n",
            "114/114 [==============================] - 0s 73us/step - loss: 0.3096 - val_loss: 0.2462\n",
            "Epoch 5/300\n",
            "114/114 [==============================] - 0s 73us/step - loss: 0.2773 - val_loss: 0.2152\n",
            "Epoch 6/300\n",
            "114/114 [==============================] - 0s 72us/step - loss: 0.2481 - val_loss: 0.1867\n",
            "Epoch 7/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.2210 - val_loss: 0.1611\n",
            "Epoch 8/300\n",
            "114/114 [==============================] - 0s 98us/step - loss: 0.1961 - val_loss: 0.1381\n",
            "Epoch 9/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 0.1739 - val_loss: 0.1177\n",
            "Epoch 10/300\n",
            "114/114 [==============================] - 0s 73us/step - loss: 0.1538 - val_loss: 0.0998\n",
            "Epoch 11/300\n",
            "114/114 [==============================] - 0s 97us/step - loss: 0.1359 - val_loss: 0.0843\n",
            "Epoch 12/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.1203 - val_loss: 0.0709\n",
            "Epoch 13/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.1064 - val_loss: 0.0596\n",
            "Epoch 14/300\n",
            "114/114 [==============================] - 0s 71us/step - loss: 0.0943 - val_loss: 0.0500\n",
            "Epoch 15/300\n",
            "114/114 [==============================] - 0s 78us/step - loss: 0.0843 - val_loss: 0.0421\n",
            "Epoch 16/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 0.0755 - val_loss: 0.0358\n",
            "Epoch 17/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 0.0682 - val_loss: 0.0308\n",
            "Epoch 18/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 0.0620 - val_loss: 0.0270\n",
            "Epoch 19/300\n",
            "114/114 [==============================] - 0s 72us/step - loss: 0.0572 - val_loss: 0.0242\n",
            "Epoch 20/300\n",
            "114/114 [==============================] - 0s 110us/step - loss: 0.0532 - val_loss: 0.0222\n",
            "Epoch 21/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.0501 - val_loss: 0.0209\n",
            "Epoch 22/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.0478 - val_loss: 0.0202\n",
            "Epoch 23/300\n",
            "114/114 [==============================] - 0s 104us/step - loss: 0.0460 - val_loss: 0.0200\n",
            "Epoch 24/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.0446 - val_loss: 0.0200\n",
            "Epoch 25/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.0439 - val_loss: 0.0203\n",
            "Epoch 26/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.0432 - val_loss: 0.0207\n",
            "Epoch 27/300\n",
            "114/114 [==============================] - 0s 97us/step - loss: 0.0430 - val_loss: 0.0211\n",
            "Epoch 28/300\n",
            "114/114 [==============================] - 0s 75us/step - loss: 0.0426 - val_loss: 0.0215\n",
            "Epoch 29/300\n",
            "114/114 [==============================] - 0s 73us/step - loss: 0.0426 - val_loss: 0.0219\n",
            "Epoch 30/300\n",
            "114/114 [==============================] - 0s 68us/step - loss: 0.0425 - val_loss: 0.0222\n",
            "Epoch 31/300\n",
            "114/114 [==============================] - 0s 98us/step - loss: 0.0425 - val_loss: 0.0225\n",
            "Epoch 32/300\n",
            "114/114 [==============================] - 0s 121us/step - loss: 0.0425 - val_loss: 0.0227\n",
            "Epoch 33/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.0425 - val_loss: 0.0228\n",
            "Operation 4 completed!\n",
            "RMSE of the trained model 4 is: 0.1372031394952895\n",
            "Train on 114 samples, validate on 77 samples\n",
            "Epoch 1/300\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 3.0413 - val_loss: 2.8172\n",
            "Epoch 2/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 2.9327 - val_loss: 2.7054\n",
            "Epoch 3/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 2.8268 - val_loss: 2.5961\n",
            "Epoch 4/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 2.7224 - val_loss: 2.4893\n",
            "Epoch 5/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 2.6206 - val_loss: 2.3849\n",
            "Epoch 6/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 2.5213 - val_loss: 2.2833\n",
            "Epoch 7/300\n",
            "114/114 [==============================] - 0s 72us/step - loss: 2.4241 - val_loss: 2.1845\n",
            "Epoch 8/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 2.3295 - val_loss: 2.0886\n",
            "Epoch 9/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 2.2371 - val_loss: 1.9954\n",
            "Epoch 10/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 2.1475 - val_loss: 1.9050\n",
            "Epoch 11/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 2.0601 - val_loss: 1.8173\n",
            "Epoch 12/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 1.9755 - val_loss: 1.7325\n",
            "Epoch 13/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 1.8931 - val_loss: 1.6506\n",
            "Epoch 14/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 1.8131 - val_loss: 1.5712\n",
            "Epoch 15/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 1.7364 - val_loss: 1.4948\n",
            "Epoch 16/300\n",
            "114/114 [==============================] - 0s 90us/step - loss: 1.6612 - val_loss: 1.4212\n",
            "Epoch 17/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 1.5893 - val_loss: 1.3504\n",
            "Epoch 18/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 1.5193 - val_loss: 1.2822\n",
            "Epoch 19/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 1.4521 - val_loss: 1.2165\n",
            "Epoch 20/300\n",
            "114/114 [==============================] - 0s 97us/step - loss: 1.3872 - val_loss: 1.1533\n",
            "Epoch 21/300\n",
            "114/114 [==============================] - 0s 105us/step - loss: 1.3247 - val_loss: 1.0928\n",
            "Epoch 22/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 1.2644 - val_loss: 1.0350\n",
            "Epoch 23/300\n",
            "114/114 [==============================] - 0s 80us/step - loss: 1.2065 - val_loss: 0.9797\n",
            "Epoch 24/300\n",
            "114/114 [==============================] - 0s 122us/step - loss: 1.1508 - val_loss: 0.9266\n",
            "Epoch 25/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 1.0973 - val_loss: 0.8756\n",
            "Epoch 26/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 1.0459 - val_loss: 0.8267\n",
            "Epoch 27/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.9962 - val_loss: 0.7800\n",
            "Epoch 28/300\n",
            "114/114 [==============================] - 0s 101us/step - loss: 0.9487 - val_loss: 0.7354\n",
            "Epoch 29/300\n",
            "114/114 [==============================] - 0s 149us/step - loss: 0.9032 - val_loss: 0.6930\n",
            "Epoch 30/300\n",
            "114/114 [==============================] - 0s 113us/step - loss: 0.8594 - val_loss: 0.6527\n",
            "Epoch 31/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 0.8176 - val_loss: 0.6142\n",
            "Epoch 32/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 0.7775 - val_loss: 0.5772\n",
            "Epoch 33/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 0.7392 - val_loss: 0.5419\n",
            "Epoch 34/300\n",
            "114/114 [==============================] - 0s 105us/step - loss: 0.7023 - val_loss: 0.5085\n",
            "Epoch 35/300\n",
            "114/114 [==============================] - 0s 103us/step - loss: 0.6669 - val_loss: 0.4767\n",
            "Epoch 36/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.6333 - val_loss: 0.4465\n",
            "Epoch 37/300\n",
            "114/114 [==============================] - 0s 112us/step - loss: 0.6011 - val_loss: 0.4179\n",
            "Epoch 38/300\n",
            "114/114 [==============================] - 0s 71us/step - loss: 0.5704 - val_loss: 0.3908\n",
            "Epoch 39/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.5411 - val_loss: 0.3651\n",
            "Epoch 40/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.5129 - val_loss: 0.3406\n",
            "Epoch 41/300\n",
            "114/114 [==============================] - 0s 116us/step - loss: 0.4863 - val_loss: 0.3174\n",
            "Epoch 42/300\n",
            "114/114 [==============================] - 0s 73us/step - loss: 0.4608 - val_loss: 0.2955\n",
            "Epoch 43/300\n",
            "114/114 [==============================] - 0s 75us/step - loss: 0.4363 - val_loss: 0.2749\n",
            "Epoch 44/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 0.4132 - val_loss: 0.2555\n",
            "Epoch 45/300\n",
            "114/114 [==============================] - 0s 80us/step - loss: 0.3912 - val_loss: 0.2371\n",
            "Epoch 46/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.3704 - val_loss: 0.2199\n",
            "Epoch 47/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.3505 - val_loss: 0.2037\n",
            "Epoch 48/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.3318 - val_loss: 0.1887\n",
            "Epoch 49/300\n",
            "114/114 [==============================] - 0s 78us/step - loss: 0.3140 - val_loss: 0.1746\n",
            "Epoch 50/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.2972 - val_loss: 0.1614\n",
            "Epoch 51/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.2815 - val_loss: 0.1490\n",
            "Epoch 52/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 0.2664 - val_loss: 0.1376\n",
            "Epoch 53/300\n",
            "114/114 [==============================] - 0s 185us/step - loss: 0.2523 - val_loss: 0.1269\n",
            "Epoch 54/300\n",
            "114/114 [==============================] - 0s 161us/step - loss: 0.2388 - val_loss: 0.1170\n",
            "Epoch 55/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.2264 - val_loss: 0.1076\n",
            "Epoch 56/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 0.2142 - val_loss: 0.0990\n",
            "Epoch 57/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.2029 - val_loss: 0.0909\n",
            "Epoch 58/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.1922 - val_loss: 0.0835\n",
            "Epoch 59/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 0.1821 - val_loss: 0.0767\n",
            "Epoch 60/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.1728 - val_loss: 0.0705\n",
            "Epoch 61/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.1640 - val_loss: 0.0649\n",
            "Epoch 62/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.1557 - val_loss: 0.0597\n",
            "Epoch 63/300\n",
            "114/114 [==============================] - 0s 111us/step - loss: 0.1481 - val_loss: 0.0550\n",
            "Epoch 64/300\n",
            "114/114 [==============================] - 0s 102us/step - loss: 0.1409 - val_loss: 0.0507\n",
            "Epoch 65/300\n",
            "114/114 [==============================] - 0s 69us/step - loss: 0.1343 - val_loss: 0.0468\n",
            "Epoch 66/300\n",
            "114/114 [==============================] - 0s 95us/step - loss: 0.1280 - val_loss: 0.0433\n",
            "Epoch 67/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.1222 - val_loss: 0.0403\n",
            "Epoch 68/300\n",
            "114/114 [==============================] - 0s 100us/step - loss: 0.1169 - val_loss: 0.0376\n",
            "Epoch 69/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.1120 - val_loss: 0.0351\n",
            "Epoch 70/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 0.1074 - val_loss: 0.0330\n",
            "Epoch 71/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.1031 - val_loss: 0.0310\n",
            "Epoch 72/300\n",
            "114/114 [==============================] - 0s 71us/step - loss: 0.0993 - val_loss: 0.0292\n",
            "Epoch 73/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 0.0954 - val_loss: 0.0277\n",
            "Epoch 74/300\n",
            "114/114 [==============================] - 0s 115us/step - loss: 0.0919 - val_loss: 0.0264\n",
            "Epoch 75/300\n",
            "114/114 [==============================] - 0s 72us/step - loss: 0.0886 - val_loss: 0.0252\n",
            "Epoch 76/300\n",
            "114/114 [==============================] - 0s 74us/step - loss: 0.0856 - val_loss: 0.0242\n",
            "Epoch 77/300\n",
            "114/114 [==============================] - 0s 104us/step - loss: 0.0827 - val_loss: 0.0233\n",
            "Epoch 78/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.0800 - val_loss: 0.0226\n",
            "Epoch 79/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.0774 - val_loss: 0.0220\n",
            "Epoch 80/300\n",
            "114/114 [==============================] - 0s 132us/step - loss: 0.0752 - val_loss: 0.0215\n",
            "Epoch 81/300\n",
            "114/114 [==============================] - 0s 90us/step - loss: 0.0730 - val_loss: 0.0211\n",
            "Epoch 82/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.0710 - val_loss: 0.0208\n",
            "Epoch 83/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 0.0690 - val_loss: 0.0207\n",
            "Epoch 84/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 0.0672 - val_loss: 0.0206\n",
            "Epoch 85/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.0656 - val_loss: 0.0206\n",
            "Epoch 86/300\n",
            "114/114 [==============================] - 0s 97us/step - loss: 0.0641 - val_loss: 0.0206\n",
            "Epoch 87/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.0625 - val_loss: 0.0208\n",
            "Epoch 88/300\n",
            "114/114 [==============================] - 0s 99us/step - loss: 0.0612 - val_loss: 0.0210\n",
            "Epoch 89/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 0.0601 - val_loss: 0.0212\n",
            "Epoch 90/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.0589 - val_loss: 0.0215\n",
            "Epoch 91/300\n",
            "114/114 [==============================] - 0s 78us/step - loss: 0.0579 - val_loss: 0.0218\n",
            "Epoch 92/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.0569 - val_loss: 0.0222\n",
            "Epoch 93/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.0561 - val_loss: 0.0226\n",
            "Epoch 94/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.0552 - val_loss: 0.0230\n",
            "Epoch 95/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.0544 - val_loss: 0.0235\n",
            "Operation 5 completed!\n",
            "RMSE of the trained model 5 is: 0.17480851655496704\n",
            "Train on 114 samples, validate on 77 samples\n",
            "Epoch 1/300\n",
            "114/114 [==============================] - 0s 3ms/step - loss: 0.9835 - val_loss: 0.8998\n",
            "Epoch 2/300\n",
            "114/114 [==============================] - 0s 90us/step - loss: 0.9233 - val_loss: 0.8372\n",
            "Epoch 3/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.8649 - val_loss: 0.7772\n",
            "Epoch 4/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.8089 - val_loss: 0.7198\n",
            "Epoch 5/300\n",
            "114/114 [==============================] - 0s 95us/step - loss: 0.7547 - val_loss: 0.6648\n",
            "Epoch 6/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.7030 - val_loss: 0.6123\n",
            "Epoch 7/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.6539 - val_loss: 0.5625\n",
            "Epoch 8/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.6066 - val_loss: 0.5155\n",
            "Epoch 9/300\n",
            "114/114 [==============================] - 0s 95us/step - loss: 0.5621 - val_loss: 0.4712\n",
            "Epoch 10/300\n",
            "114/114 [==============================] - 0s 90us/step - loss: 0.5200 - val_loss: 0.4297\n",
            "Epoch 11/300\n",
            "114/114 [==============================] - 0s 99us/step - loss: 0.4800 - val_loss: 0.3910\n",
            "Epoch 12/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.4427 - val_loss: 0.3547\n",
            "Epoch 13/300\n",
            "114/114 [==============================] - 0s 90us/step - loss: 0.4075 - val_loss: 0.3209\n",
            "Epoch 14/300\n",
            "114/114 [==============================] - 0s 95us/step - loss: 0.3744 - val_loss: 0.2892\n",
            "Epoch 15/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.3433 - val_loss: 0.2597\n",
            "Epoch 16/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.3144 - val_loss: 0.2324\n",
            "Epoch 17/300\n",
            "114/114 [==============================] - 0s 99us/step - loss: 0.2872 - val_loss: 0.2073\n",
            "Epoch 18/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.2625 - val_loss: 0.1844\n",
            "Epoch 19/300\n",
            "114/114 [==============================] - 0s 104us/step - loss: 0.2390 - val_loss: 0.1638\n",
            "Epoch 20/300\n",
            "114/114 [==============================] - 0s 75us/step - loss: 0.2180 - val_loss: 0.1449\n",
            "Epoch 21/300\n",
            "114/114 [==============================] - 0s 80us/step - loss: 0.1988 - val_loss: 0.1280\n",
            "Epoch 22/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.1810 - val_loss: 0.1128\n",
            "Epoch 23/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.1650 - val_loss: 0.0992\n",
            "Epoch 24/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 0.1505 - val_loss: 0.0871\n",
            "Epoch 25/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.1372 - val_loss: 0.0764\n",
            "Epoch 26/300\n",
            "114/114 [==============================] - 0s 90us/step - loss: 0.1256 - val_loss: 0.0670\n",
            "Epoch 27/300\n",
            "114/114 [==============================] - 0s 97us/step - loss: 0.1148 - val_loss: 0.0589\n",
            "Epoch 28/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.1053 - val_loss: 0.0518\n",
            "Epoch 29/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 0.0970 - val_loss: 0.0456\n",
            "Epoch 30/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.0894 - val_loss: 0.0403\n",
            "Epoch 31/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.0828 - val_loss: 0.0358\n",
            "Epoch 32/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.0769 - val_loss: 0.0321\n",
            "Epoch 33/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 0.0716 - val_loss: 0.0289\n",
            "Epoch 34/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.0673 - val_loss: 0.0263\n",
            "Epoch 35/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 0.0632 - val_loss: 0.0243\n",
            "Epoch 36/300\n",
            "114/114 [==============================] - 0s 111us/step - loss: 0.0598 - val_loss: 0.0227\n",
            "Epoch 37/300\n",
            "114/114 [==============================] - 0s 137us/step - loss: 0.0568 - val_loss: 0.0214\n",
            "Epoch 38/300\n",
            "114/114 [==============================] - 0s 100us/step - loss: 0.0543 - val_loss: 0.0205\n",
            "Epoch 39/300\n",
            "114/114 [==============================] - 0s 113us/step - loss: 0.0521 - val_loss: 0.0199\n",
            "Epoch 40/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.0503 - val_loss: 0.0196\n",
            "Epoch 41/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 0.0488 - val_loss: 0.0194\n",
            "Epoch 42/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.0475 - val_loss: 0.0194\n",
            "Epoch 43/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.0465 - val_loss: 0.0195\n",
            "Epoch 44/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.0456 - val_loss: 0.0197\n",
            "Epoch 45/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 0.0448 - val_loss: 0.0200\n",
            "Epoch 46/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.0442 - val_loss: 0.0204\n",
            "Epoch 47/300\n",
            "114/114 [==============================] - 0s 97us/step - loss: 0.0438 - val_loss: 0.0208\n",
            "Epoch 48/300\n",
            "114/114 [==============================] - 0s 103us/step - loss: 0.0434 - val_loss: 0.0212\n",
            "Epoch 49/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 0.0431 - val_loss: 0.0216\n",
            "Epoch 50/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.0429 - val_loss: 0.0220\n",
            "Epoch 51/300\n",
            "114/114 [==============================] - 0s 78us/step - loss: 0.0427 - val_loss: 0.0223\n",
            "Epoch 52/300\n",
            "114/114 [==============================] - 0s 108us/step - loss: 0.0426 - val_loss: 0.0226\n",
            "Operation 6 completed!\n",
            "RMSE of the trained model 6 is: 0.1435485410496119\n",
            "Train on 114 samples, validate on 77 samples\n",
            "Epoch 1/300\n",
            "114/114 [==============================] - 0s 3ms/step - loss: 1.0103 - val_loss: 0.7969\n",
            "Epoch 2/300\n",
            "114/114 [==============================] - 0s 103us/step - loss: 0.9477 - val_loss: 0.7367\n",
            "Epoch 3/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 0.8869 - val_loss: 0.6792\n",
            "Epoch 4/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.8290 - val_loss: 0.6244\n",
            "Epoch 5/300\n",
            "114/114 [==============================] - 0s 75us/step - loss: 0.7730 - val_loss: 0.5724\n",
            "Epoch 6/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.7195 - val_loss: 0.5232\n",
            "Epoch 7/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 0.6689 - val_loss: 0.4767\n",
            "Epoch 8/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 0.6201 - val_loss: 0.4331\n",
            "Epoch 9/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.5739 - val_loss: 0.3918\n",
            "Epoch 10/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.5305 - val_loss: 0.3531\n",
            "Epoch 11/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 0.4888 - val_loss: 0.3171\n",
            "Epoch 12/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 0.4500 - val_loss: 0.2837\n",
            "Epoch 13/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 0.4134 - val_loss: 0.2529\n",
            "Epoch 14/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.3791 - val_loss: 0.2246\n",
            "Epoch 15/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.3474 - val_loss: 0.1987\n",
            "Epoch 16/300\n",
            "114/114 [==============================] - 0s 116us/step - loss: 0.3178 - val_loss: 0.1753\n",
            "Epoch 17/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.2903 - val_loss: 0.1540\n",
            "Epoch 18/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.2648 - val_loss: 0.1348\n",
            "Epoch 19/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 0.2419 - val_loss: 0.1175\n",
            "Epoch 20/300\n",
            "114/114 [==============================] - 0s 97us/step - loss: 0.2204 - val_loss: 0.1022\n",
            "Epoch 21/300\n",
            "114/114 [==============================] - 0s 156us/step - loss: 0.2006 - val_loss: 0.0886\n",
            "Epoch 22/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 0.1826 - val_loss: 0.0766\n",
            "Epoch 23/300\n",
            "114/114 [==============================] - 0s 95us/step - loss: 0.1668 - val_loss: 0.0660\n",
            "Epoch 24/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.1518 - val_loss: 0.0570\n",
            "Epoch 25/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 0.1384 - val_loss: 0.0492\n",
            "Epoch 26/300\n",
            "114/114 [==============================] - 0s 75us/step - loss: 0.1267 - val_loss: 0.0426\n",
            "Epoch 27/300\n",
            "114/114 [==============================] - 0s 78us/step - loss: 0.1159 - val_loss: 0.0372\n",
            "Epoch 28/300\n",
            "114/114 [==============================] - 0s 74us/step - loss: 0.1065 - val_loss: 0.0327\n",
            "Epoch 29/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.0978 - val_loss: 0.0291\n",
            "Epoch 30/300\n",
            "114/114 [==============================] - 0s 109us/step - loss: 0.0906 - val_loss: 0.0261\n",
            "Epoch 31/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 0.0840 - val_loss: 0.0239\n",
            "Epoch 32/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.0782 - val_loss: 0.0223\n",
            "Epoch 33/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.0731 - val_loss: 0.0212\n",
            "Epoch 34/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.0688 - val_loss: 0.0206\n",
            "Epoch 35/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.0650 - val_loss: 0.0202\n",
            "Epoch 36/300\n",
            "114/114 [==============================] - 0s 102us/step - loss: 0.0616 - val_loss: 0.0202\n",
            "Epoch 37/300\n",
            "114/114 [==============================] - 0s 98us/step - loss: 0.0586 - val_loss: 0.0205\n",
            "Epoch 38/300\n",
            "114/114 [==============================] - 0s 100us/step - loss: 0.0562 - val_loss: 0.0210\n",
            "Epoch 39/300\n",
            "114/114 [==============================] - 0s 104us/step - loss: 0.0539 - val_loss: 0.0216\n",
            "Epoch 40/300\n",
            "114/114 [==============================] - 0s 139us/step - loss: 0.0520 - val_loss: 0.0224\n",
            "Epoch 41/300\n",
            "114/114 [==============================] - 0s 105us/step - loss: 0.0505 - val_loss: 0.0233\n",
            "Epoch 42/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.0490 - val_loss: 0.0243\n",
            "Epoch 43/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.0481 - val_loss: 0.0254\n",
            "Epoch 44/300\n",
            "114/114 [==============================] - 0s 73us/step - loss: 0.0471 - val_loss: 0.0265\n",
            "Epoch 45/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.0462 - val_loss: 0.0275\n",
            "Epoch 46/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.0456 - val_loss: 0.0285\n",
            "Operation 7 completed!\n",
            "RMSE of the trained model 7 is: 0.16499079450979778\n",
            "Train on 114 samples, validate on 77 samples\n",
            "Epoch 1/300\n",
            "114/114 [==============================] - 0s 3ms/step - loss: 1.2794 - val_loss: 1.3506\n",
            "Epoch 2/300\n",
            "114/114 [==============================] - 0s 90us/step - loss: 1.2076 - val_loss: 1.2708\n",
            "Epoch 3/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 1.1378 - val_loss: 1.1936\n",
            "Epoch 4/300\n",
            "114/114 [==============================] - 0s 75us/step - loss: 1.0710 - val_loss: 1.1194\n",
            "Epoch 5/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 1.0061 - val_loss: 1.0481\n",
            "Epoch 6/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.9444 - val_loss: 0.9798\n",
            "Epoch 7/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.8843 - val_loss: 0.9146\n",
            "Epoch 8/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.8272 - val_loss: 0.8520\n",
            "Epoch 9/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.7725 - val_loss: 0.7921\n",
            "Epoch 10/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 0.7207 - val_loss: 0.7352\n",
            "Epoch 11/300\n",
            "114/114 [==============================] - 0s 90us/step - loss: 0.6709 - val_loss: 0.6815\n",
            "Epoch 12/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 0.6237 - val_loss: 0.6308\n",
            "Epoch 13/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.5791 - val_loss: 0.5827\n",
            "Epoch 14/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.5376 - val_loss: 0.5375\n",
            "Epoch 15/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 0.4979 - val_loss: 0.4953\n",
            "Epoch 16/300\n",
            "114/114 [==============================] - 0s 107us/step - loss: 0.4604 - val_loss: 0.4557\n",
            "Epoch 17/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.4257 - val_loss: 0.4184\n",
            "Epoch 18/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.3930 - val_loss: 0.3833\n",
            "Epoch 19/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.3623 - val_loss: 0.3506\n",
            "Epoch 20/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.3337 - val_loss: 0.3202\n",
            "Epoch 21/300\n",
            "114/114 [==============================] - 0s 98us/step - loss: 0.3067 - val_loss: 0.2919\n",
            "Epoch 22/300\n",
            "114/114 [==============================] - 0s 103us/step - loss: 0.2819 - val_loss: 0.2656\n",
            "Epoch 23/300\n",
            "114/114 [==============================] - 0s 108us/step - loss: 0.2589 - val_loss: 0.2413\n",
            "Epoch 24/300\n",
            "114/114 [==============================] - 0s 107us/step - loss: 0.2376 - val_loss: 0.2189\n",
            "Epoch 25/300\n",
            "114/114 [==============================] - 0s 99us/step - loss: 0.2182 - val_loss: 0.1987\n",
            "Epoch 26/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.1999 - val_loss: 0.1803\n",
            "Epoch 27/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.1840 - val_loss: 0.1634\n",
            "Epoch 28/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.1689 - val_loss: 0.1481\n",
            "Epoch 29/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.1553 - val_loss: 0.1340\n",
            "Epoch 30/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 0.1430 - val_loss: 0.1212\n",
            "Epoch 31/300\n",
            "114/114 [==============================] - 0s 95us/step - loss: 0.1317 - val_loss: 0.1097\n",
            "Epoch 32/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 0.1212 - val_loss: 0.0991\n",
            "Epoch 33/300\n",
            "114/114 [==============================] - 0s 99us/step - loss: 0.1122 - val_loss: 0.0895\n",
            "Epoch 34/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 0.1037 - val_loss: 0.0811\n",
            "Epoch 35/300\n",
            "114/114 [==============================] - 0s 146us/step - loss: 0.0962 - val_loss: 0.0735\n",
            "Epoch 36/300\n",
            "114/114 [==============================] - 0s 144us/step - loss: 0.0894 - val_loss: 0.0668\n",
            "Epoch 37/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.0836 - val_loss: 0.0609\n",
            "Epoch 38/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.0782 - val_loss: 0.0556\n",
            "Epoch 39/300\n",
            "114/114 [==============================] - 0s 99us/step - loss: 0.0736 - val_loss: 0.0510\n",
            "Epoch 40/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.0694 - val_loss: 0.0469\n",
            "Epoch 41/300\n",
            "114/114 [==============================] - 0s 126us/step - loss: 0.0656 - val_loss: 0.0432\n",
            "Epoch 42/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 0.0624 - val_loss: 0.0399\n",
            "Epoch 43/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.0596 - val_loss: 0.0370\n",
            "Epoch 44/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.0569 - val_loss: 0.0345\n",
            "Epoch 45/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.0547 - val_loss: 0.0323\n",
            "Epoch 46/300\n",
            "114/114 [==============================] - 0s 107us/step - loss: 0.0527 - val_loss: 0.0304\n",
            "Epoch 47/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.0509 - val_loss: 0.0287\n",
            "Epoch 48/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 0.0494 - val_loss: 0.0273\n",
            "Epoch 49/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.0481 - val_loss: 0.0260\n",
            "Epoch 50/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 0.0469 - val_loss: 0.0249\n",
            "Epoch 51/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 0.0459 - val_loss: 0.0239\n",
            "Epoch 52/300\n",
            "114/114 [==============================] - 0s 136us/step - loss: 0.0450 - val_loss: 0.0231\n",
            "Epoch 53/300\n",
            "114/114 [==============================] - 0s 131us/step - loss: 0.0443 - val_loss: 0.0224\n",
            "Epoch 54/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.0436 - val_loss: 0.0218\n",
            "Epoch 55/300\n",
            "114/114 [==============================] - 0s 102us/step - loss: 0.0430 - val_loss: 0.0212\n",
            "Epoch 56/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.0426 - val_loss: 0.0208\n",
            "Epoch 57/300\n",
            "114/114 [==============================] - 0s 95us/step - loss: 0.0422 - val_loss: 0.0205\n",
            "Epoch 58/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.0418 - val_loss: 0.0202\n",
            "Epoch 59/300\n",
            "114/114 [==============================] - 0s 114us/step - loss: 0.0416 - val_loss: 0.0200\n",
            "Epoch 60/300\n",
            "114/114 [==============================] - 0s 148us/step - loss: 0.0414 - val_loss: 0.0198\n",
            "Epoch 61/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.0412 - val_loss: 0.0197\n",
            "Epoch 62/300\n",
            "114/114 [==============================] - 0s 90us/step - loss: 0.0411 - val_loss: 0.0196\n",
            "Epoch 63/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 0.0410 - val_loss: 0.0195\n",
            "Epoch 64/300\n",
            "114/114 [==============================] - 0s 99us/step - loss: 0.0409 - val_loss: 0.0194\n",
            "Epoch 65/300\n",
            "114/114 [==============================] - 0s 75us/step - loss: 0.0408 - val_loss: 0.0194\n",
            "Epoch 66/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 0.0408 - val_loss: 0.0193\n",
            "Epoch 67/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.0407 - val_loss: 0.0193\n",
            "Epoch 68/300\n",
            "114/114 [==============================] - 0s 100us/step - loss: 0.0406 - val_loss: 0.0192\n",
            "Epoch 69/300\n",
            "114/114 [==============================] - 0s 106us/step - loss: 0.0406 - val_loss: 0.0192\n",
            "Epoch 70/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.0405 - val_loss: 0.0192\n",
            "Epoch 71/300\n",
            "114/114 [==============================] - 0s 99us/step - loss: 0.0405 - val_loss: 0.0191\n",
            "Epoch 72/300\n",
            "114/114 [==============================] - 0s 114us/step - loss: 0.0405 - val_loss: 0.0191\n",
            "Epoch 73/300\n",
            "114/114 [==============================] - 0s 99us/step - loss: 0.0404 - val_loss: 0.0191\n",
            "Epoch 74/300\n",
            "114/114 [==============================] - 0s 93us/step - loss: 0.0404 - val_loss: 0.0191\n",
            "Epoch 75/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.0404 - val_loss: 0.0191\n",
            "Epoch 76/300\n",
            "114/114 [==============================] - 0s 98us/step - loss: 0.0404 - val_loss: 0.0191\n",
            "Epoch 77/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.0404 - val_loss: 0.0191\n",
            "Epoch 78/300\n",
            "114/114 [==============================] - 0s 80us/step - loss: 0.0404 - val_loss: 0.0191\n",
            "Epoch 79/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.0404 - val_loss: 0.0191\n",
            "Epoch 80/300\n",
            "114/114 [==============================] - 0s 98us/step - loss: 0.0404 - val_loss: 0.0191\n",
            "Epoch 81/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.0404 - val_loss: 0.0192\n",
            "Epoch 82/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 0.0404 - val_loss: 0.0192\n",
            "Epoch 83/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.0404 - val_loss: 0.0192\n",
            "Epoch 84/300\n",
            "114/114 [==============================] - 0s 95us/step - loss: 0.0404 - val_loss: 0.0192\n",
            "Epoch 85/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.0404 - val_loss: 0.0192\n",
            "Operation 8 completed!\n",
            "RMSE of the trained model 8 is: 0.14020007141642712\n",
            "Train on 114 samples, validate on 77 samples\n",
            "Epoch 1/300\n",
            "114/114 [==============================] - 0s 3ms/step - loss: 1.2711 - val_loss: 1.1783\n",
            "Epoch 2/300\n",
            "114/114 [==============================] - 0s 101us/step - loss: 1.2002 - val_loss: 1.1042\n",
            "Epoch 3/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 1.1321 - val_loss: 1.0327\n",
            "Epoch 4/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 1.0660 - val_loss: 0.9640\n",
            "Epoch 5/300\n",
            "114/114 [==============================] - 0s 95us/step - loss: 1.0021 - val_loss: 0.8980\n",
            "Epoch 6/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.9409 - val_loss: 0.8348\n",
            "Epoch 7/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.8815 - val_loss: 0.7743\n",
            "Epoch 8/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.8252 - val_loss: 0.7167\n",
            "Epoch 9/300\n",
            "114/114 [==============================] - 0s 95us/step - loss: 0.7713 - val_loss: 0.6621\n",
            "Epoch 10/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.7196 - val_loss: 0.6106\n",
            "Epoch 11/300\n",
            "114/114 [==============================] - 0s 95us/step - loss: 0.6704 - val_loss: 0.5619\n",
            "Epoch 12/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 0.6241 - val_loss: 0.5159\n",
            "Epoch 13/300\n",
            "114/114 [==============================] - 0s 118us/step - loss: 0.5797 - val_loss: 0.4724\n",
            "Epoch 14/300\n",
            "114/114 [==============================] - 0s 73us/step - loss: 0.5378 - val_loss: 0.4314\n",
            "Epoch 15/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 0.4982 - val_loss: 0.3927\n",
            "Epoch 16/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.4604 - val_loss: 0.3566\n",
            "Epoch 17/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 0.4255 - val_loss: 0.3230\n",
            "Epoch 18/300\n",
            "114/114 [==============================] - 0s 80us/step - loss: 0.3923 - val_loss: 0.2920\n",
            "Epoch 19/300\n",
            "114/114 [==============================] - 0s 77us/step - loss: 0.3616 - val_loss: 0.2636\n",
            "Epoch 20/300\n",
            "114/114 [==============================] - 0s 80us/step - loss: 0.3327 - val_loss: 0.2373\n",
            "Epoch 21/300\n",
            "114/114 [==============================] - 0s 107us/step - loss: 0.3065 - val_loss: 0.2131\n",
            "Epoch 22/300\n",
            "114/114 [==============================] - 0s 102us/step - loss: 0.2817 - val_loss: 0.1910\n",
            "Epoch 23/300\n",
            "114/114 [==============================] - 0s 133us/step - loss: 0.2587 - val_loss: 0.1707\n",
            "Epoch 24/300\n",
            "114/114 [==============================] - 0s 127us/step - loss: 0.2376 - val_loss: 0.1522\n",
            "Epoch 25/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 0.2182 - val_loss: 0.1353\n",
            "Epoch 26/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 0.2001 - val_loss: 0.1201\n",
            "Epoch 27/300\n",
            "114/114 [==============================] - 0s 89us/step - loss: 0.1837 - val_loss: 0.1064\n",
            "Epoch 28/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.1687 - val_loss: 0.0943\n",
            "Epoch 29/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.1551 - val_loss: 0.0835\n",
            "Epoch 30/300\n",
            "114/114 [==============================] - 0s 104us/step - loss: 0.1429 - val_loss: 0.0741\n",
            "Epoch 31/300\n",
            "114/114 [==============================] - 0s 150us/step - loss: 0.1315 - val_loss: 0.0657\n",
            "Epoch 32/300\n",
            "114/114 [==============================] - 0s 110us/step - loss: 0.1219 - val_loss: 0.0583\n",
            "Epoch 33/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.1126 - val_loss: 0.0518\n",
            "Epoch 34/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.1045 - val_loss: 0.0462\n",
            "Epoch 35/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.0971 - val_loss: 0.0413\n",
            "Epoch 36/300\n",
            "114/114 [==============================] - 0s 76us/step - loss: 0.0905 - val_loss: 0.0370\n",
            "Epoch 37/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 0.0847 - val_loss: 0.0334\n",
            "Epoch 38/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.0793 - val_loss: 0.0304\n",
            "Epoch 39/300\n",
            "114/114 [==============================] - 0s 96us/step - loss: 0.0745 - val_loss: 0.0278\n",
            "Epoch 40/300\n",
            "114/114 [==============================] - 0s 97us/step - loss: 0.0703 - val_loss: 0.0256\n",
            "Epoch 41/300\n",
            "114/114 [==============================] - 0s 90us/step - loss: 0.0666 - val_loss: 0.0238\n",
            "Epoch 42/300\n",
            "114/114 [==============================] - 0s 78us/step - loss: 0.0632 - val_loss: 0.0224\n",
            "Epoch 43/300\n",
            "114/114 [==============================] - 0s 84us/step - loss: 0.0603 - val_loss: 0.0213\n",
            "Epoch 44/300\n",
            "114/114 [==============================] - 0s 79us/step - loss: 0.0577 - val_loss: 0.0205\n",
            "Epoch 45/300\n",
            "114/114 [==============================] - 0s 72us/step - loss: 0.0555 - val_loss: 0.0199\n",
            "Epoch 46/300\n",
            "114/114 [==============================] - 0s 73us/step - loss: 0.0535 - val_loss: 0.0196\n",
            "Epoch 47/300\n",
            "114/114 [==============================] - 0s 72us/step - loss: 0.0519 - val_loss: 0.0194\n",
            "Epoch 48/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.0504 - val_loss: 0.0193\n",
            "Epoch 49/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.0491 - val_loss: 0.0193\n",
            "Epoch 50/300\n",
            "114/114 [==============================] - 0s 94us/step - loss: 0.0480 - val_loss: 0.0194\n",
            "Epoch 51/300\n",
            "114/114 [==============================] - 0s 105us/step - loss: 0.0471 - val_loss: 0.0197\n",
            "Epoch 52/300\n",
            "114/114 [==============================] - 0s 92us/step - loss: 0.0463 - val_loss: 0.0199\n",
            "Epoch 53/300\n",
            "114/114 [==============================] - 0s 74us/step - loss: 0.0456 - val_loss: 0.0203\n",
            "Epoch 54/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.0450 - val_loss: 0.0206\n",
            "Epoch 55/300\n",
            "114/114 [==============================] - 0s 91us/step - loss: 0.0445 - val_loss: 0.0210\n",
            "Epoch 56/300\n",
            "114/114 [==============================] - 0s 107us/step - loss: 0.0441 - val_loss: 0.0214\n",
            "Epoch 57/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.0437 - val_loss: 0.0218\n",
            "Epoch 58/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.0434 - val_loss: 0.0222\n",
            "Operation 9 completed!\n",
            "RMSE of the trained model 9 is: 0.14753736896969719\n",
            "Train on 114 samples, validate on 77 samples\n",
            "Epoch 1/300\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 0.2194 - val_loss: 0.1673\n",
            "Epoch 2/300\n",
            "114/114 [==============================] - 0s 105us/step - loss: 0.1939 - val_loss: 0.1430\n",
            "Epoch 3/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.1708 - val_loss: 0.1211\n",
            "Epoch 4/300\n",
            "114/114 [==============================] - 0s 86us/step - loss: 0.1493 - val_loss: 0.1016\n",
            "Epoch 5/300\n",
            "114/114 [==============================] - 0s 78us/step - loss: 0.1302 - val_loss: 0.0844\n",
            "Epoch 6/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.1133 - val_loss: 0.0695\n",
            "Epoch 7/300\n",
            "114/114 [==============================] - 0s 72us/step - loss: 0.0983 - val_loss: 0.0568\n",
            "Epoch 8/300\n",
            "114/114 [==============================] - 0s 82us/step - loss: 0.0855 - val_loss: 0.0464\n",
            "Epoch 9/300\n",
            "114/114 [==============================] - 0s 74us/step - loss: 0.0748 - val_loss: 0.0380\n",
            "Epoch 10/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.0658 - val_loss: 0.0315\n",
            "Epoch 11/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.0585 - val_loss: 0.0266\n",
            "Epoch 12/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.0527 - val_loss: 0.0231\n",
            "Epoch 13/300\n",
            "114/114 [==============================] - 0s 87us/step - loss: 0.0486 - val_loss: 0.0208\n",
            "Epoch 14/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.0452 - val_loss: 0.0195\n",
            "Epoch 15/300\n",
            "114/114 [==============================] - 0s 81us/step - loss: 0.0430 - val_loss: 0.0189\n",
            "Epoch 16/300\n",
            "114/114 [==============================] - 0s 78us/step - loss: 0.0416 - val_loss: 0.0190\n",
            "Epoch 17/300\n",
            "114/114 [==============================] - 0s 83us/step - loss: 0.0409 - val_loss: 0.0196\n",
            "Epoch 18/300\n",
            "114/114 [==============================] - 0s 75us/step - loss: 0.0406 - val_loss: 0.0203\n",
            "Epoch 19/300\n",
            "114/114 [==============================] - 0s 97us/step - loss: 0.0406 - val_loss: 0.0210\n",
            "Epoch 20/300\n",
            "114/114 [==============================] - 0s 85us/step - loss: 0.0407 - val_loss: 0.0216\n",
            "Epoch 21/300\n",
            "114/114 [==============================] - 0s 98us/step - loss: 0.0409 - val_loss: 0.0223\n",
            "Epoch 22/300\n",
            "114/114 [==============================] - 0s 97us/step - loss: 0.0412 - val_loss: 0.0228\n",
            "Epoch 23/300\n",
            "114/114 [==============================] - 0s 88us/step - loss: 0.0414 - val_loss: 0.0231\n",
            "Epoch 24/300\n",
            "114/114 [==============================] - 0s 106us/step - loss: 0.0415 - val_loss: 0.0232\n",
            "Epoch 25/300\n",
            "114/114 [==============================] - 0s 110us/step - loss: 0.0416 - val_loss: 0.0231\n",
            "Operation 10 completed!\n",
            "RMSE of the trained model 10 is: 0.1359067443583101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkbBQ8PQdE2W",
        "colab_type": "text"
      },
      "source": [
        "5. Validating the invidual trained models using test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_4A-nQpc9JF",
        "colab_type": "code",
        "outputId": "3e33ed51-a165-4734-e440-cc5c8c748760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_index_end= int(len(predictors)*0.8)\n",
        "test_index_end= len(predictors)\n",
        "\n",
        "\n",
        "X_train_pre= predictors[0:train_index_end]\n",
        "X_test_pre= predictors[train_index_end:test_index_end]\n",
        "y_train_pre= target[0:train_index_end]\n",
        "y_test_pre= target[train_index_end:test_index_end]\n",
        "\n",
        "X_norm= MinMaxScaler()\n",
        "Y_norm= MinMaxScaler()\n",
        "\n",
        "X_train= X_norm.fit_transform(X_train_pre)\n",
        "X_test= X_norm.transform(X_test_pre)\n",
        "y_train= Y_norm.fit_transform(y_train_pre.reshape(-1,1))\n",
        "y_test= Y_norm.transform(y_test_pre.reshape(-1,1))\n",
        "\n",
        "\n",
        "for i in range(1,11):\n",
        "  model_train= locals() ['model_train_' + str(i)]\n",
        "  y_diff=[]\n",
        "  y_pred= model_train.predict(X_test)\n",
        "  \n",
        "  print('model_train_' + str(i))\n",
        "  print('\\n')\n",
        "  for j in range(len(y_pred)):\n",
        "    print('y_pred: ' + str(y_pred[j]) + '|' + 'y_test: ' + str(y_test[j]))\n",
        "    y_diff.append(float(y_test[j]- y_pred[j]))\n",
        "    \n",
        "  print(MSE(y_test,y_pred)**0.5)\n",
        "  pd.Series(y_diff).hist()\n",
        "  plt.show()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_train_1\n",
            "\n",
            "\n",
            "y_pred: [0.49909472]|y_test: [0.82453399]\n",
            "y_pred: [0.50569075]|y_test: [0.51924743]\n",
            "y_pred: [0.5117471]|y_test: [0.30989464]\n",
            "y_pred: [0.49657482]|y_test: [0.72539204]\n",
            "y_pred: [0.4980001]|y_test: [0.48632996]\n",
            "y_pred: [0.5476536]|y_test: [0.61381076]\n",
            "y_pred: [0.5317451]|y_test: [0.50688718]\n",
            "y_pred: [0.49038577]|y_test: [0.69599409]\n",
            "y_pred: [0.470554]|y_test: [0.60533423]\n",
            "y_pred: [0.49529243]|y_test: [0.46610205]\n",
            "y_pred: [0.482821]|y_test: [0.36758053]\n",
            "y_pred: [0.51623267]|y_test: [0.6261693]\n",
            "y_pred: [0.51636463]|y_test: [0.41651457]\n",
            "y_pred: [0.5224672]|y_test: [0.52326547]\n",
            "y_pred: [0.5360296]|y_test: [0.51913501]\n",
            "y_pred: [0.55682164]|y_test: [0.58509699]\n",
            "y_pred: [0.5386604]|y_test: [0.41203001]\n",
            "y_pred: [0.557786]|y_test: [0.74217919]\n",
            "y_pred: [0.5614118]|y_test: [0.58911503]\n",
            "y_pred: [0.51776975]|y_test: [0.51920347]\n",
            "y_pred: [0.5195398]|y_test: [0.39582924]\n",
            "y_pred: [0.5402525]|y_test: [0.45310578]\n",
            "y_pred: [0.53753567]|y_test: [0.61413117]\n",
            "y_pred: [0.53729564]|y_test: [0.68418563]\n",
            "y_pred: [0.5386824]|y_test: [0.78662425]\n",
            "y_pred: [0.50448984]|y_test: [0.59707166]\n",
            "y_pred: [0.5071786]|y_test: [0.58065705]\n",
            "y_pred: [0.5109923]|y_test: [0.57245761]\n",
            "y_pred: [0.47497773]|y_test: [0.65817209]\n",
            "y_pred: [0.50855196]|y_test: [0.62123752]\n",
            "y_pred: [0.5172336]|y_test: [0.55605906]\n",
            "y_pred: [0.562604]|y_test: [0.40567081]\n",
            "y_pred: [0.5813192]|y_test: [0.44602162]\n",
            "y_pred: [0.5642532]|y_test: [0.41721547]\n",
            "y_pred: [0.56115645]|y_test: [0.35131943]\n",
            "y_pred: [0.55344594]|y_test: [0.62613987]\n",
            "y_pred: [0.5595649]|y_test: [0.95451542]\n",
            "y_pred: [0.5304089]|y_test: [0.58049435]\n",
            "y_pred: [0.5299425]|y_test: [0.86108642]\n",
            "y_pred: [0.53591496]|y_test: [0.51962765]\n",
            "y_pred: [0.5855013]|y_test: [0.43878966]\n",
            "y_pred: [0.59454834]|y_test: [0.57626652]\n",
            "y_pred: [0.5949595]|y_test: [0.58433903]\n",
            "y_pred: [0.60152346]|y_test: [0.41849529]\n",
            "y_pred: [0.6045737]|y_test: [0.34921533]\n",
            "y_pred: [0.6073314]|y_test: [0.51128347]\n",
            "y_pred: [0.632137]|y_test: [0.52348616]\n",
            "y_pred: [0.6162746]|y_test: [0.4297047]\n",
            "0.1506224346615768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_train_2\n",
            "\n",
            "\n",
            "y_pred: [0.5369168]|y_test: [0.82453399]\n",
            "y_pred: [0.5317444]|y_test: [0.51924743]\n",
            "y_pred: [0.5280821]|y_test: [0.30989464]\n",
            "y_pred: [0.5377429]|y_test: [0.72539204]\n",
            "y_pred: [0.5364035]|y_test: [0.48632996]\n",
            "y_pred: [0.5072233]|y_test: [0.61381076]\n",
            "y_pred: [0.51615816]|y_test: [0.50688718]\n",
            "y_pred: [0.5399929]|y_test: [0.69599409]\n",
            "y_pred: [0.55150306]|y_test: [0.60533423]\n",
            "y_pred: [0.535797]|y_test: [0.46610205]\n",
            "y_pred: [0.5423902]|y_test: [0.36758053]\n",
            "y_pred: [0.5231457]|y_test: [0.6261693]\n",
            "y_pred: [0.5209335]|y_test: [0.41651457]\n",
            "y_pred: [0.51610136]|y_test: [0.52326547]\n",
            "y_pred: [0.50719464]|y_test: [0.51913501]\n",
            "y_pred: [0.49367476]|y_test: [0.58509699]\n",
            "y_pred: [0.50521773]|y_test: [0.41203001]\n",
            "y_pred: [0.49400207]|y_test: [0.74217919]\n",
            "y_pred: [0.49116293]|y_test: [0.58911503]\n",
            "y_pred: [0.51715887]|y_test: [0.51920347]\n",
            "y_pred: [0.51631236]|y_test: [0.39582924]\n",
            "y_pred: [0.50415295]|y_test: [0.45310578]\n",
            "y_pred: [0.5059118]|y_test: [0.61413117]\n",
            "y_pred: [0.505532]|y_test: [0.68418563]\n",
            "y_pred: [0.5052527]|y_test: [0.78662425]\n",
            "y_pred: [0.52516186]|y_test: [0.59707166]\n",
            "y_pred: [0.523337]|y_test: [0.58065705]\n",
            "y_pred: [0.52091753]|y_test: [0.57245761]\n",
            "y_pred: [0.5395706]|y_test: [0.65817209]\n",
            "y_pred: [0.51700574]|y_test: [0.62123752]\n",
            "y_pred: [0.5114118]|y_test: [0.55605906]\n",
            "y_pred: [0.48439094]|y_test: [0.40567081]\n",
            "y_pred: [0.47094277]|y_test: [0.44602162]\n",
            "y_pred: [0.48074785]|y_test: [0.41721547]\n",
            "y_pred: [0.48068076]|y_test: [0.35131943]\n",
            "y_pred: [0.4869821]|y_test: [0.62613987]\n",
            "y_pred: [0.48061487]|y_test: [0.95451542]\n",
            "y_pred: [0.49547902]|y_test: [0.58049435]\n",
            "y_pred: [0.49468958]|y_test: [0.86108642]\n",
            "y_pred: [0.49331674]|y_test: [0.51962765]\n",
            "y_pred: [0.46000445]|y_test: [0.43878966]\n",
            "y_pred: [0.45457235]|y_test: [0.57626652]\n",
            "y_pred: [0.44881365]|y_test: [0.58433903]\n",
            "y_pred: [0.4457012]|y_test: [0.41849529]\n",
            "y_pred: [0.44178334]|y_test: [0.34921533]\n",
            "y_pred: [0.43735698]|y_test: [0.51128347]\n",
            "y_pred: [0.42495793]|y_test: [0.52348616]\n",
            "y_pred: [0.43301448]|y_test: [0.4297047]\n",
            "0.14448359291723062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD8CAYAAACINTRsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_train_3\n",
            "\n",
            "\n",
            "y_pred: [0.5735657]|y_test: [0.82453399]\n",
            "y_pred: [0.5702174]|y_test: [0.51924743]\n",
            "y_pred: [0.5687042]|y_test: [0.30989464]\n",
            "y_pred: [0.5731971]|y_test: [0.72539204]\n",
            "y_pred: [0.57215583]|y_test: [0.48632996]\n",
            "y_pred: [0.56104124]|y_test: [0.61381076]\n",
            "y_pred: [0.5639752]|y_test: [0.50688718]\n",
            "y_pred: [0.57261974]|y_test: [0.69599409]\n",
            "y_pred: [0.57695925]|y_test: [0.60533423]\n",
            "y_pred: [0.56962675]|y_test: [0.46610205]\n",
            "y_pred: [0.5714356]|y_test: [0.36758053]\n",
            "y_pred: [0.5643789]|y_test: [0.6261693]\n",
            "y_pred: [0.56127435]|y_test: [0.41651457]\n",
            "y_pred: [0.55809456]|y_test: [0.52326547]\n",
            "y_pred: [0.5536724]|y_test: [0.51913501]\n",
            "y_pred: [0.54710907]|y_test: [0.58509699]\n",
            "y_pred: [0.5524548]|y_test: [0.41203001]\n",
            "y_pred: [0.5481944]|y_test: [0.74217919]\n",
            "y_pred: [0.5463524]|y_test: [0.58911503]\n",
            "y_pred: [0.55671096]|y_test: [0.51920347]\n",
            "y_pred: [0.5565858]|y_test: [0.39582924]\n",
            "y_pred: [0.5519088]|y_test: [0.45310578]\n",
            "y_pred: [0.5527567]|y_test: [0.61413117]\n",
            "y_pred: [0.55205685]|y_test: [0.68418563]\n",
            "y_pred: [0.5525194]|y_test: [0.78662425]\n",
            "y_pred: [0.5600387]|y_test: [0.59707166]\n",
            "y_pred: [0.5590704]|y_test: [0.58065705]\n",
            "y_pred: [0.5579395]|y_test: [0.57245761]\n",
            "y_pred: [0.56273764]|y_test: [0.65817209]\n",
            "y_pred: [0.5508437]|y_test: [0.62123752]\n",
            "y_pred: [0.5481263]|y_test: [0.55605906]\n",
            "y_pred: [0.53725755]|y_test: [0.40567081]\n",
            "y_pred: [0.5294962]|y_test: [0.44602162]\n",
            "y_pred: [0.53300714]|y_test: [0.41721547]\n",
            "y_pred: [0.5309749]|y_test: [0.35131943]\n",
            "y_pred: [0.53528655]|y_test: [0.62613987]\n",
            "y_pred: [0.52988803]|y_test: [0.95451542]\n",
            "y_pred: [0.5333165]|y_test: [0.58049435]\n",
            "y_pred: [0.5318986]|y_test: [0.86108642]\n",
            "y_pred: [0.5335823]|y_test: [0.51962765]\n",
            "y_pred: [0.5162581]|y_test: [0.43878966]\n",
            "y_pred: [0.51406336]|y_test: [0.57626652]\n",
            "y_pred: [0.5059958]|y_test: [0.58433903]\n",
            "y_pred: [0.505605]|y_test: [0.41849529]\n",
            "y_pred: [0.5018579]|y_test: [0.34921533]\n",
            "y_pred: [0.4972004]|y_test: [0.51128347]\n",
            "y_pred: [0.49487138]|y_test: [0.52348616]\n",
            "y_pred: [0.49653482]|y_test: [0.4297047]\n",
            "0.13510406088724272\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_train_4\n",
            "\n",
            "\n",
            "y_pred: [0.5670165]|y_test: [0.82453399]\n",
            "y_pred: [0.5661643]|y_test: [0.51924743]\n",
            "y_pred: [0.5679891]|y_test: [0.30989464]\n",
            "y_pred: [0.56459296]|y_test: [0.72539204]\n",
            "y_pred: [0.5638778]|y_test: [0.48632996]\n",
            "y_pred: [0.58101624]|y_test: [0.61381076]\n",
            "y_pred: [0.5744715]|y_test: [0.50688718]\n",
            "y_pred: [0.55918616]|y_test: [0.69599409]\n",
            "y_pred: [0.5522017]|y_test: [0.60533423]\n",
            "y_pred: [0.5577128]|y_test: [0.46610205]\n",
            "y_pred: [0.55177426]|y_test: [0.36758053]\n",
            "y_pred: [0.5640035]|y_test: [0.6261693]\n",
            "y_pred: [0.5589278]|y_test: [0.41651457]\n",
            "y_pred: [0.55802035]|y_test: [0.52326547]\n",
            "y_pred: [0.56041443]|y_test: [0.51913501]\n",
            "y_pred: [0.5644599]|y_test: [0.58509699]\n",
            "y_pred: [0.56028175]|y_test: [0.41203001]\n",
            "y_pred: [0.56695616]|y_test: [0.74217919]\n",
            "y_pred: [0.5665048]|y_test: [0.58911503]\n",
            "y_pred: [0.5523399]|y_test: [0.51920347]\n",
            "y_pred: [0.55340445]|y_test: [0.39582924]\n",
            "y_pred: [0.56051904]|y_test: [0.45310578]\n",
            "y_pred: [0.5599749]|y_test: [0.61413117]\n",
            "y_pred: [0.55863875]|y_test: [0.68418563]\n",
            "y_pred: [0.560405]|y_test: [0.78662425]\n",
            "y_pred: [0.54833263]|y_test: [0.59707166]\n",
            "y_pred: [0.5486527]|y_test: [0.58065705]\n",
            "y_pred: [0.5495115]|y_test: [0.57245761]\n",
            "y_pred: [0.5315983]|y_test: [0.65817209]\n",
            "y_pred: [0.53593856]|y_test: [0.62123752]\n",
            "y_pred: [0.537666]|y_test: [0.55605906]\n",
            "y_pred: [0.5522601]|y_test: [0.40567081]\n",
            "y_pred: [0.5528471]|y_test: [0.44602162]\n",
            "y_pred: [0.54638904]|y_test: [0.41721547]\n",
            "y_pred: [0.5407844]|y_test: [0.35131943]\n",
            "y_pred: [0.5423936]|y_test: [0.62613987]\n",
            "y_pred: [0.53783315]|y_test: [0.95451542]\n",
            "y_pred: [0.5225272]|y_test: [0.58049435]\n",
            "y_pred: [0.5198341]|y_test: [0.86108642]\n",
            "y_pred: [0.5269363]|y_test: [0.51962765]\n",
            "y_pred: [0.53387654]|y_test: [0.43878966]\n",
            "y_pred: [0.5367467]|y_test: [0.57626652]\n",
            "y_pred: [0.5236386]|y_test: [0.58433903]\n",
            "y_pred: [0.527716]|y_test: [0.41849529]\n",
            "y_pred: [0.5236835]|y_test: [0.34921533]\n",
            "y_pred: [0.5179242]|y_test: [0.51128347]\n",
            "y_pred: [0.53190774]|y_test: [0.52348616]\n",
            "y_pred: [0.5232554]|y_test: [0.4297047]\n",
            "0.1372031394952895\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAD7BJREFUeJzt3X2MZfVdx/H3t6zIw9AFpLnFhXRs\ngiTIVFqu9aFJmXExUhZpoyQFgbCKmWitJbqm2QZNE00j1dBIIrFuWlKqhmm60pRASwuUKzYBdBaR\nZaHloa6W7bK0VrYdukon/frHXJqZ8c7ep3OffrxfyWTPOfd3z/nMzZ3Pnjlz7+9GZiJJmnyvGXUA\nSVI1LHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSITYN82CnnXZaTk9PD/OQXXvp\npZc48cQTRx2jK5OYGSYzt5mHw8xr7dmz51uZ+bp244Za6NPT0ywuLg7zkF1rNBrMzs6OOkZXJjEz\nTGZuMw+HmdeKiP/oZJyXXCSpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRBD\nfaeo1M70zrtGctz9N2wbyXGlKnmGLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqE\nhS5JhbDQJakQFrokFaJtoUfELRHxQkQ8vmrbX0TEVyLisYj4TEScPNiYkqR2OjlD/wRw0bpt9wDn\nZuabgKeAD1ScS5LUpbaFnpkPAN9et+2LmbncXH0IOGMA2SRJXajiGvpvAp+vYD+SpD5EZrYfFDEN\n3JmZ567bfj1QB341N9hRRMwD8wC1Wu38hYWFPiMP1tLSElNTU6OO0ZVJzAytc+89cHgkWWa2bO5o\n3CQ+1mYejkFmnpub25OZ9Xbjev6Ai4jYDlwCbN2ozAEycxewC6Ber+fs7GyvhxyKRqPBuGdcbxIz\nQ+vc20f1ARdXzrYdA5P5WJt5OMYhc0+FHhEXAe8HLsjM71UbSZLUi05etngb8CBwdkQ8FxHXAn8F\nnATcExGPRsRHB5xTktRG2zP0zLyixeaPDyCLJKkPvlNUkgphoUtSISx0SSqEhS5JhbDQJakQFrok\nFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1Ih\nLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEK0LfSIuCUiXoiIx1dtOzUi7omIp5v/njLYmJKkdjo5Q/8E\ncNG6bTuB+zLzLOC+5rokaYTaFnpmPgB8e93mdwK3NpdvBd5VcS5JUpd6vYZey8yDzeXngVpFeSRJ\nPYrMbD8oYhq4MzPPba6/mJknr7r9vzOz5XX0iJgH5gFqtdr5CwsLFcQenKWlJaampkYdoyuTmBla\n59574PBIssxs2dzRuEl8rM08HIPMPDc3tycz6+3Gbepx/4ci4vTMPBgRpwMvbDQwM3cBuwDq9XrO\nzs72eMjhaDQajHvG9SYxM7TOvX3nXSPJsv/K2bZjYDIfazMPxzhk7vWSyx3ANc3la4DPVhNHktSr\nTl62eBvwIHB2RDwXEdcCNwC/FBFPAxc21yVJI9T2kktmXrHBTVsrziJJ6oPvFJWkQljoklQIC12S\nCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQ\nFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEH0VekT8fkTsi4jHI+K2iDiuqmCS\npO70XOgRsQV4H1DPzHOBY4DLqwomSepOv5dcNgHHR8Qm4ATgG/1HkiT1IjKz9ztHXAd8CDgCfDEz\nr2wxZh6YB6jVaucvLCz0fLxhWFpaYmpqas22vQcOjyhNZ2rHw6Ej1e1vZsvm6nZ2FOP0WHf6PbfK\nPO7MPByDzDw3N7cnM+vtxvVc6BFxCvAPwLuBF4FPA7sz8+82uk+9Xs/FxcWejjcsjUaD2dnZNdum\nd941mjAd2jGzzI17N1W2v/03bKtsX0czTo91p99zq8zjzszDMcjMEdFRofdzyeVC4N8z85uZ+X3g\nduAX+tifJKkP/RT6fwI/FxEnREQAW4Enq4klSepWz4WemQ8Du4FHgL3Nfe2qKJckqUt9XXjNzA8C\nH6woiySpD75TVJIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1Ihqptz\nVcUY1hS2O2aW2T7mUxNLk8QzdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJ\nKoSFLkmFsNAlqRAWuiQVoq9Cj4iTI2J3RHwlIp6MiJ+vKpgkqTv9zrZ4E3B3Zl4WEccCJ1SQSZLU\ng54LPSI2A28HtgNk5svAy9XEkiR1KzKztztGnAfsAp4AfhrYA1yXmS+tGzcPzAPUarXzFxYW+go8\naEtLS0xNTa3ZtvfA4RGl6UzteDh0ZNQpujdOuWe2bO5oXKvnx7gz83AMMvPc3NyezKy3G9dPodeB\nh4C3ZebDEXET8J3M/OON7lOv13NxcbGn4w1Lo9FgdnZ2zbZhfeBDr3bMLHPj3sn7rJJxyr3/hm0d\njWv1/Bh3Zh6OQWaOiI4KvZ8/ij4HPJeZDzfXdwNv6WN/kqQ+9Fzomfk88PWIOLu5aSsrl18kSSPQ\n7++7vwf8ffMVLl8DfqP/SJKkXvRV6Jn5KND2uo4kafB8p6gkFcJCl6RCWOiSVAgLXZIKYaFLUiEs\ndEkqhIUuSYWw0CWpEBa6JBXCQpekQozH3KXSiHU6RfKOmWW2VzydcqdT90rteIYuSYWw0CWpEBa6\nJBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVou9Cj4hjIuJfI+LOKgJJknpT\nxRn6dcCTFexHktSHvgo9Is4AtgEfqyaOJKlX/Z6h/yXwfuAHFWSRJPUhMrO3O0ZcAlycme+JiFng\nDzPzkhbj5oF5gFqtdv7CwkIfcQdvaWmJqampNdv2Hjg8ojSdqR0Ph46MOkX3JjH3IDLPbNlc7Q7X\nafWcHndmXmtubm5PZtbbjeun0P8MuBpYBo4DXgvcnplXbXSfer2ei4uLPR1vWBqNBrOzs2u2dfrh\nB6OyY2aZG/dO3meVTGLuQWQe9AdctHpOjzszrxURHRV6z5dcMvMDmXlGZk4DlwNfOlqZS5IGy9eh\nS1IhKvndMTMbQKOKfUmSeuMZuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrok\nFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1Ih\nLHRJKoSFLkmF6LnQI+LMiLg/Ip6IiH0RcV2VwSRJ3dnUx32XgR2Z+UhEnATsiYh7MvOJirJJkrrQ\n8xl6Zh7MzEeay98FngS2VBVMktSdSq6hR8Q08Gbg4Sr2J0nqXmRmfzuImAL+EfhQZt7e4vZ5YB6g\nVqudv7Cw0NfxBm1paYmpqak12/YeODyiNJ2pHQ+Hjow6RfcmMbeZqzGzZfNRb2/1czjuBpl5bm5u\nT2bW243rq9Aj4keAO4EvZOZH2o2v1+u5uLjY8/GGodFoMDs7u2bb9M67RhOmQztmlrlxbz9/DhmN\nScxt5mrsv2HbUW9v9XM47gaZOSI6KvR+XuUSwMeBJzspc0nSYPVzDf1twNXAL0bEo82viyvKJUnq\nUs+/h2Xml4GoMIskqQ++U1SSCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgph\noUtSISx0SSrEeM2peRTDmsJ2x8wy28d8ulxp0rX7eZ7En8N2mdtNGVwFz9AlqRAWuiQVwkKXpEJY\n6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RC9FXoEXFRRHw1Ip6JiJ1VhZIkda/n\nQo+IY4CbgXcA5wBXRMQ5VQWTJHWnnzP0twLPZObXMvNlYAF4ZzWxJEnd6qfQtwBfX7X+XHObJGkE\nIjN7u2PEZcBFmflbzfWrgZ/NzPeuGzcPzDdXzwa+2nvcoTgN+NaoQ3RpEjPDZOY283CYea03ZObr\n2g3q5wMuDgBnrlo/o7ltjczcBezq4zhDFRGLmVkfdY5uTGJmmMzcZh4OM/emn0su/wKcFRE/ERHH\nApcDd1QTS5LUrZ7P0DNzOSLeC3wBOAa4JTP3VZZMktSVvj5TNDM/B3yuoizjYmIuD60yiZlhMnOb\neTjM3IOe/ygqSRovvvVfkgrxqi/0iDg1Iu6JiKeb/57SYsx5EfFgROyLiMci4t2jyLoqT9vMzXF3\nR8SLEXHnsDOuynDU6SEi4kcj4lPN2x+OiOnhp/x/mdplfntEPBIRy82X745cB5n/ICKeaD5/74uI\nN4wi53od5P7tiNgbEY9GxJfH4d3onU55EhG/FhEZEcN75Utmvqq/gD8HdjaXdwIfbjHmJ4Gzmss/\nDhwETh7nzM3btgK/Atw5opzHAM8CbwSOBf4NOGfdmPcAH20uXw58asTPh04yTwNvAj4JXDbKvF1k\nngNOaC7/zqgf5y5yv3bV8qXA3eOeuTnuJOAB4CGgPqx8r/ozdFamK7i1uXwr8K71AzLzqcx8urn8\nDeAFoO2L/AeobWaAzLwP+O6wQrXQyfQQq7+X3cDWiIghZlyvbebM3J+ZjwE/GEXAFjrJfH9mfq+5\n+hAr7xsZtU5yf2fV6onAqP/o1+mUJ38KfBj4n2GGs9ChlpkHm8vPA7WjDY6It7LyP/Ozgw52FF1l\nHqFOpof44ZjMXAYOAz82lHStTeKUFt1mvhb4/EATdaaj3BHxuxHxLCu/mb5vSNk20jZzRLwFODMz\n7xpmMOjzZYuTIiLuBV7f4qbrV69kZkbEhmcAEXE68LfANZk50LOzqjJLq0XEVUAduGDUWTqVmTcD\nN0fErwN/BFwz4kgbiojXAB8Bto/i+K+KQs/MCze6LSIORcTpmXmwWdgvbDDutcBdwPWZ+dCAov5Q\nFZnHQCfTQ7wy5rmI2ARsBv5rOPFa6mhKizHTUeaIuJCVE4ILMvN/h5TtaLp9rBeAvx5oovbaZT4J\nOBdoNK8cvh64IyIuzczFQYfzksvKdAWv/I9/DfDZ9QOaUxt8BvhkZu4eYraNtM08JjqZHmL193IZ\n8KVs/lVpRCZxSou2mSPizcDfAJdm5ricAHSS+6xVq9uAp4eYr5WjZs7Mw5l5WmZOZ+Y0K3+vGEqZ\nvxLgVf3FyvXa+1h5otwLnNrcXgc+1ly+Cvg+8Oiqr/PGOXNz/Z+AbwJHWLnW98sjyHox8BQrf3O4\nvrntT1h5kgMcB3waeAb4Z+CNY/CcaJf5Z5qP50us/DaxbwIy3wscWvX8vWPUmTvMfROwr5n5fuCn\nxj3zurENhvgqF98pKkmF8JKLJBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRD/BzWT\n3mtyVkXOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_train_5\n",
            "\n",
            "\n",
            "y_pred: [0.5257034]|y_test: [0.82453399]\n",
            "y_pred: [0.5167469]|y_test: [0.51924743]\n",
            "y_pred: [0.51301384]|y_test: [0.30989464]\n",
            "y_pred: [0.5243882]|y_test: [0.72539204]\n",
            "y_pred: [0.52153915]|y_test: [0.48632996]\n",
            "y_pred: [0.49470592]|y_test: [0.61381076]\n",
            "y_pred: [0.5014739]|y_test: [0.50688718]\n",
            "y_pred: [0.5221056]|y_test: [0.69599409]\n",
            "y_pred: [0.5326034]|y_test: [0.60533423]\n",
            "y_pred: [0.5139936]|y_test: [0.46610205]\n",
            "y_pred: [0.517949]|y_test: [0.36758053]\n",
            "y_pred: [0.5010177]|y_test: [0.6261693]\n",
            "y_pred: [0.49208283]|y_test: [0.41651457]\n",
            "y_pred: [0.48353913]|y_test: [0.52326547]\n",
            "y_pred: [0.4721771]|y_test: [0.51913501]\n",
            "y_pred: [0.4553839]|y_test: [0.58509699]\n",
            "y_pred: [0.46893317]|y_test: [0.41203001]\n",
            "y_pred: [0.45863017]|y_test: [0.74217919]\n",
            "y_pred: [0.45368266]|y_test: [0.58911503]\n",
            "y_pred: [0.47906524]|y_test: [0.51920347]\n",
            "y_pred: [0.4788852]|y_test: [0.39582924]\n",
            "y_pred: [0.4675216]|y_test: [0.45310578]\n",
            "y_pred: [0.46968818]|y_test: [0.61413117]\n",
            "y_pred: [0.46764028]|y_test: [0.68418563]\n",
            "y_pred: [0.46912205]|y_test: [0.78662425]\n",
            "y_pred: [0.48731473]|y_test: [0.59707166]\n",
            "y_pred: [0.48479477]|y_test: [0.58065705]\n",
            "y_pred: [0.4819196]|y_test: [0.57245761]\n",
            "y_pred: [0.49216443]|y_test: [0.65817209]\n",
            "y_pred: [0.46120292]|y_test: [0.62123752]\n",
            "y_pred: [0.45422915]|y_test: [0.55605906]\n",
            "y_pred: [0.42744762]|y_test: [0.40567081]\n",
            "y_pred: [0.40690038]|y_test: [0.44602162]\n",
            "y_pred: [0.41529375]|y_test: [0.41721547]\n",
            "y_pred: [0.40907735]|y_test: [0.35131943]\n",
            "y_pred: [0.4207794]|y_test: [0.62613987]\n",
            "y_pred: [0.40576065]|y_test: [0.95451542]\n",
            "y_pred: [0.41272983]|y_test: [0.58049435]\n",
            "y_pred: [0.40857852]|y_test: [0.86108642]\n",
            "y_pred: [0.41405356]|y_test: [0.51962765]\n",
            "y_pred: [0.36886618]|y_test: [0.43878966]\n",
            "y_pred: [0.36342397]|y_test: [0.57626652]\n",
            "y_pred: [0.3399968]|y_test: [0.58433903]\n",
            "y_pred: [0.3395341]|y_test: [0.41849529]\n",
            "y_pred: [0.32894063]|y_test: [0.34921533]\n",
            "y_pred: [0.31566396]|y_test: [0.51128347]\n",
            "y_pred: [0.31143132]|y_test: [0.52348616]\n",
            "y_pred: [0.31463987]|y_test: [0.4297047]\n",
            "0.17480851655496704\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEDlJREFUeJzt3X+M5Hddx/Hnm57ItVuuxZIFrw0r\nSa3BLiI34g8S2KU1VoqFSBOKQHoKWRURomfIkWpINAZQSySBiJdCKEq6hANC7QFSStdKQqt3tbC0\nBQrYQI9yBZGDLSe44e0f+726Xe86M9/vd2c/9/H5SCb7/c58vt/vK7M7r/3Od2a+E5mJJOnU96it\nDiBJ6oeFLkmVsNAlqRIWuiRVwkKXpEpY6JJUCQtdkiphoUtSJSx0SarEtklu7JxzzsmZmZnWyz/4\n4IOcccYZ/QXaBKVnLD0flJ+x9HxQfsbS80FZGQ8dOvTNzHz80IGZObHLrl27soubb7650/KTUHrG\n0vNllp+x9HyZ5WcsPV9mWRmBgzlCx3rIRZIqYaFLUiUsdEmqhIUuSZWw0CWpEkMLPSLeGREPRMRn\n1133lxHxuYj4TER8MCLO2tyYkqRhRtlDfxdwyYbrbgQuzMynAl8AXtdzLknSmIYWembeAnxrw3Uf\ny8zVZvZW4NxNyCZJGkMfx9B/C/hID+uRJHUQOcKXREfEDHBDZl644fqrgAHw63mSFUXEArAAMD09\nvWtxcbF12JWVFaamplovPwmlZyw93/Lho0xvhyPHJrvd2Z07Rh5b+n0I5WcsPR+UlXF+fv5QZg6G\njWt9LpeI2A08D7joZGUOkJn7gH0Ag8Eg5+bm2m6SpaUluiw/CaVnLD3f7r0H2DO7ytXLEz3NEPe+\nZG7ksaXfh1B+xtLzwamRcaNWj5qIuAR4LfDszPxev5EkSW2M8rbF64BPARdExH0R8XLgrcCZwI0R\ncUdEvH2Tc0qShhi6h56ZLz7B1e/YhCySpA78pKgkVcJCl6RKWOiSVAkLXZIqYaFLUiUsdEmqhIUu\nSZWw0CWpEha6JFXCQpekSljoklQJC12SKmGhS1IlLHRJqoSFLkmVsNAlqRIWuiRVwkKXpEpY6JJU\nCQtdkiphoUtSJSx0SaqEhS5JlbDQJakSFrokVcJCl6RKDC30iHhnRDwQEZ9dd93jIuLGiLin+Xn2\n5saUJA0zyh76u4BLNly3F7gpM88HbmrmJUlbaGihZ+YtwLc2XP184Npm+lrgBT3nkiSNKTJz+KCI\nGeCGzLywmf92Zp7VTAfwn8fnT7DsArAAMD09vWtxcbF12JWVFaamplovPwmlZyw93/Lho0xvhyPH\nJrvd2Z07Rh5b+n0I5WcsPR+UlXF+fv5QZg6GjdvWdUOZmRFx0v8KmbkP2AcwGAxybm6u9baWlpbo\nsvwklJ6x9Hy79x5gz+wqVy93/tMcy70vmRt5bOn3IZSfsfR8cGpk3Kjtu1yORMQTAZqfD/QXSZLU\nRttCvx64spm+EvhQP3EkSW2N8rbF64BPARdExH0R8XLgjcAvR8Q9wMXNvCRpCw09UJmZLz7JTRf1\nnEWS1IGfFJWkSljoklQJC12SKmGhS1IlLHRJqoSFLkmVsNAlqRIWuiRVwkKXpEpY6JJUCQtdkiox\n2ZNO65Qws/fAVkeQ1IJ76JJUCQtdkiphoUtSJSx0SaqEhS5JlbDQJakSFrokVcJCl6RKWOiSVAkL\nXZIqYaFLUiUsdEmqhIUuSZXoVOgR8QcRcWdEfDYirouIx/QVTJI0ntaFHhE7gVcDg8y8EDgNuKKv\nYJKk8XQ95LIN2B4R24DTga91jyRJaqN1oWfmYeCvgK8A9wNHM/NjfQWTJI0nMrPdghFnA+8HXgR8\nG3gfsD8z/37DuAVgAWB6enrX4uJi67ArKytMTU21Xn4S+sy4fPhoL+tZb3o7HDnW+2p7VXrGvvPN\n7tzR38oapT9WSs8HZWWcn58/lJmDYeO6fAXdxcC/Z+Y3ACLiA8AvAQ8r9MzcB+wDGAwGOTc313qD\nS0tLdFl+EvrMuHsTvgpuz+wqVy+X/c2DpWfsO9+9L5nrbV3Hlf5YKT0fnBoZN+pyDP0rwC9ExOkR\nEcBFwN39xJIkjavLMfTbgP3A7cBys659PeWSJI2p0/PGzHw98PqeskiSOvCTopJUCQtdkiphoUtS\nJSx0SaqEhS5JlbDQJakSFrokVcJCl6RKWOiSVAkLXZIqYaFLUiUsdEmqhIUuSZWw0CWpEha6JFXC\nQpekSljoklQJC12SKmGhS1IlLHRJqoSFLkmVsNAlqRIWuiRVwkKXpEpY6JJUCQtdkirRqdAj4qyI\n2B8Rn4uIuyPiF/sKJkkaz7aOy78F+GhmXh4RjwZO7yGTJKmF1oUeETuAZwG7ATLzB8AP+oklSRpX\nZGa7BSOeBuwD7gJ+BjgEvCYzH9wwbgFYAJient61uLjYOuzKygpTU1Otl5+EPjMuHz7ay3rWm94O\nR471vtpelZ6x73yzO3f0t7JG6Y+V0vNBWRnn5+cPZeZg2LguhT4AbgWemZm3RcRbgO9k5p+cbJnB\nYJAHDx5stT2ApaUl5ubmWi8/CX1mnNl7oJf1rLdndpWrl7seadtcpWfsO9+9b7y0t3UdV/pjpfR8\nUFbGiBip0Lu8KHofcF9m3tbM7wee3mF9kqQOWhd6Zn4d+GpEXNBcdRFrh18kSVug6/PG3wfe07zD\n5cvAb3aPJElqo1OhZ+YdwNDjOpKkzecnRSWpEha6JFXCQpekSljoklQJC12SKmGhS1IlLHRJqoSF\nLkmVsNAlqRIWuiRVwkKXpEpY6JJUCQtdkiphoUtSJSx0SaqEhS5JlbDQJakSFrokVcJCl6RKWOiS\nVAkLXZIqYaFLUiUsdEmqhIUuSZWw0CWpEp0LPSJOi4h/i4gb+ggkSWqnjz301wB397AeSVIHnQo9\nIs4FLgWu6SeOJKmtrnvofw28FvhhD1kkSR1EZrZbMOJ5wHMz85URMQf8UWY+7wTjFoAFgOnp6V2L\ni4utw66srDA1NdV6+baWDx8deez0djhybBPDdFR6Pig/Y9/5Znfu6G9lja16rIyq9HxQVsb5+flD\nmTkYNq5Lob8BeBmwCjwGeCzwgcx86cmWGQwGefDgwVbbA1haWmJubq718m3N7D0w8tg9s6tcvbxt\nE9N0U3o+KD9j3/nufeOlva3ruK16rIyq9HxQVsaIGKnQWx9yyczXZea5mTkDXAF84pHKXJK0uXwf\nuiRVopfnjZm5BCz1sS5JUjvuoUtSJSx0SaqEhS5JlbDQJakSFrokVcJCl6RKWOiSVAkLXZIqYaFL\nUiUsdEmqhIUuSZWw0CWpEuWedFr6f2Kc8+2Pas/sKruHrHczzsOureUeuiRVwkKXpEpY6JJUCQtd\nkiphoUtSJSx0SaqEhS5JlbDQJakSFrokVcJCl6RKWOiSVAkLXZIqYaFLUiVaF3pEnBcRN0fEXRFx\nZ0S8ps9gkqTxdDl97iqwJzNvj4gzgUMRcWNm3tVTNknSGFrvoWfm/Zl5ezP9XeBuYGdfwSRJ44nM\n7L6SiBngFuDCzPzOhtsWgAWA6enpXYuLi622sXz4KNPb4cixblk3W+kZS88H5WcsPR+MlnF2547J\nhDmBlZUVpqamtmz7oygp4/z8/KHMHAwb17nQI2IK+CfgzzPzA480djAY5MGDB1ttZ2bvAfbMrnL1\nctlfslR6xtLzQfkZS88Ho2Xcym8sWlpaYm5ubsu2P4qSMkbESIXe6V0uEfEjwPuB9wwrc0nS5ury\nLpcA3gHcnZlv7i+SJKmNLnvozwReBjwnIu5oLs/tKZckaUytDwRm5ieB6DGLJKkDPykqSZWw0CWp\nEha6JFXCQpekSljoklQJC12SKmGhS1IlLHRJqoSFLkmVsNAlqRIWuiRVwkKXpEqUfZZ+SVVaPnyU\n3XsPbHWMR7RndrXXjJP4QhH30CWpEha6JFXCQpekSljoklQJC12SKmGhS1IlLHRJqoSFLkmVsNAl\nqRIWuiRVwkKXpEpY6JJUiU6FHhGXRMTnI+KLEbG3r1CSpPG1LvSIOA14G/CrwFOAF0fEU/oKJkka\nT5c99GcAX8zML2fmD4BF4Pn9xJIkjatLoe8Evrpu/r7mOknSFojMbLdgxOXAJZn5imb+ZcDPZ+ar\nNoxbABaa2QuAz7ePyznANzssPwmlZyw9H5SfsfR8UH7G0vNBWRmflJmPHzaoyzcWHQbOWzd/bnPd\nw2TmPmBfh+08JCIOZuagj3VtltIzlp4Pys9Yej4oP2Pp+eDUyLhRl0Mu/wqcHxE/ERGPBq4Aru8n\nliRpXK330DNzNSJeBfwjcBrwzsy8s7dkkqSxdPqS6Mz8MPDhnrKMopdDN5us9Iyl54PyM5aeD8rP\nWHo+ODUyPkzrF0UlSWXxo/+SVImiCz0iHhcRN0bEPc3Ps08w5mkR8amIuDMiPhMRLyotYzPuoxHx\n7Yi4YUK5HvG0DBHxoxHx3ub22yJiZhK5xsz4rIi4PSJWm7fJlpbvDyPirubv7qaIeFKBGX8nIpYj\n4o6I+OSkP8096ulBIuKFEZERMdF3lYxw/+2OiG80998dEfGKSeYbW2YWewH+AtjbTO8F3nSCMT8J\nnN9M/zhwP3BWSRmb2y4Cfg24YQKZTgO+BDwZeDTwaeApG8a8Enh7M30F8N4J/25HyTgDPBV4N3B5\ngfnmgdOb6d8t9D587Lrpy4CPlpSvGXcmcAtwKzAoKR+wG3jrJH+vXS5F76GzdiqBa5vpa4EXbByQ\nmV/IzHua6a8BDwBD34Dfo6EZATLzJuC7E8o0ymkZ1ufeD1wUETGhfCNlzMx7M/MzwA8nmGucfDdn\n5vea2VtZ+yxGaRm/s272DGCSL5qNenqQPwPeBPzXBLNBhacvKb3QpzPz/mb668D0Iw2OiGew9p/2\nS5sdbJ2xMk7IKKdleGhMZq4CR4Efm0i6DdtvlHbqiHHzvRz4yKYm+r9GyhgRvxcRX2Lt2eSrJ5QN\nRsgXEU8HzsvMAxPMddyov+MXNofV9kfEeSe4vRid3rbYh4j4OPCEE9x01fqZzMyIOOneRUQ8Efg7\n4MrM7HWPrq+MqlNEvBQYAM/e6iwnkplvA94WEb8B/DFw5RZHAiAiHgW8mbXDGqX6B+C6zPx+RPw2\na89qn7PFmU5qyws9My8+2W0RcSQinpiZ9zeF/cBJxj0WOABclZm3lphxwkY5LcPxMfdFxDZgB/Af\nk4n3sO0fd8JTR2yhkfJFxMWs/WN/dmZ+f0LZjhv3PlwE/mZTEz3csHxnAhcCS83RvicA10fEZZl5\nsIB8ZOb6x8Q1rD3LKVbph1yu53/3Jq4EPrRxQHPagQ8C787M/RPMdtzQjFtglNMyrM99OfCJbF4F\nKijjVhqaLyJ+Fvhb4LLM3Ip/5KNkPH/d7KXAPaXky8yjmXlOZs5k5gxrr0NMqsyH5oOHnvkfdxlw\n94SytbPVr8o+0oW1Y7o3sfZH+HHgcc31A+CaZvqlwH8Dd6y7PK2kjM38PwPfAI6xdqzuVzY513OB\nL7D2esJVzXV/ytoDBuAxwPuALwL/Ajx5C36/wzL+XHNfPcjas4c7C8v3ceDIur+76wu8D98C3Nnk\nuxn46ZLybRi7xATf5TLi/feG5v77dHP//dSkf8fjXPykqCRVovRDLpKkEVnoklQJC12SKmGhS1Il\nLHRJqoSFLkmVsNAlqRIWuiRV4n8AnGhnGM4fNHkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_train_6\n",
            "\n",
            "\n",
            "y_pred: [0.5535435]|y_test: [0.82453399]\n",
            "y_pred: [0.54539484]|y_test: [0.51924743]\n",
            "y_pred: [0.5376687]|y_test: [0.30989464]\n",
            "y_pred: [0.55691314]|y_test: [0.72539204]\n",
            "y_pred: [0.5552025]|y_test: [0.48632996]\n",
            "y_pred: [0.49166074]|y_test: [0.61381076]\n",
            "y_pred: [0.5121126]|y_test: [0.50688718]\n",
            "y_pred: [0.5651414]|y_test: [0.69599409]\n",
            "y_pred: [0.5905455]|y_test: [0.60533423]\n",
            "y_pred: [0.55915445]|y_test: [0.46610205]\n",
            "y_pred: [0.57528096]|y_test: [0.36758053]\n",
            "y_pred: [0.5324225]|y_test: [0.6261693]\n",
            "y_pred: [0.53270507]|y_test: [0.41651457]\n",
            "y_pred: [0.52514446]|y_test: [0.52326547]\n",
            "y_pred: [0.5079477]|y_test: [0.51913501]\n",
            "y_pred: [0.4815285]|y_test: [0.58509699]\n",
            "y_pred: [0.50465995]|y_test: [0.41203001]\n",
            "y_pred: [0.48011774]|y_test: [0.74217919]\n",
            "y_pred: [0.47560075]|y_test: [0.58911503]\n",
            "y_pred: [0.53151757]|y_test: [0.51920347]\n",
            "y_pred: [0.5292084]|y_test: [0.39582924]\n",
            "y_pred: [0.50264335]|y_test: [0.45310578]\n",
            "y_pred: [0.5060959]|y_test: [0.61413117]\n",
            "y_pred: [0.5065078]|y_test: [0.68418563]\n",
            "y_pred: [0.504622]|y_test: [0.78662425]\n",
            "y_pred: [0.5485001]|y_test: [0.59707166]\n",
            "y_pred: [0.545105]|y_test: [0.58065705]\n",
            "y_pred: [0.5402531]|y_test: [0.57245761]\n",
            "y_pred: [0.58695555]|y_test: [0.65817209]\n",
            "y_pred: [0.54447865]|y_test: [0.62123752]\n",
            "y_pred: [0.53343385]|y_test: [0.55605906]\n",
            "y_pred: [0.47522175]|y_test: [0.40567081]\n",
            "y_pred: [0.4516021]|y_test: [0.44602162]\n",
            "y_pred: [0.47360027]|y_test: [0.41721547]\n",
            "y_pred: [0.47793382]|y_test: [0.35131943]\n",
            "y_pred: [0.48752016]|y_test: [0.62613987]\n",
            "y_pred: [0.4801668]|y_test: [0.95451542]\n",
            "y_pred: [0.51809275]|y_test: [0.58049435]\n",
            "y_pred: [0.5188932]|y_test: [0.86108642]\n",
            "y_pred: [0.5108099]|y_test: [0.51962765]\n",
            "y_pred: [0.4477415]|y_test: [0.43878966]\n",
            "y_pred: [0.43609872]|y_test: [0.57626652]\n",
            "y_pred: [0.43654358]|y_test: [0.58433903]\n",
            "y_pred: [0.42794243]|y_test: [0.41849529]\n",
            "y_pred: [0.4243789]|y_test: [0.34921533]\n",
            "y_pred: [0.42131135]|y_test: [0.51128347]\n",
            "y_pred: [0.38887846]|y_test: [0.52348616]\n",
            "y_pred: [0.40959987]|y_test: [0.4297047]\n",
            "0.1435485410496119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_train_7\n",
            "\n",
            "\n",
            "y_pred: [0.5586392]|y_test: [0.82453399]\n",
            "y_pred: [0.5462782]|y_test: [0.51924743]\n",
            "y_pred: [0.5372893]|y_test: [0.30989464]\n",
            "y_pred: [0.5608581]|y_test: [0.72539204]\n",
            "y_pred: [0.5577078]|y_test: [0.48632996]\n",
            "y_pred: [0.48589334]|y_test: [0.61381076]\n",
            "y_pred: [0.5079717]|y_test: [0.50688718]\n",
            "y_pred: [0.56681174]|y_test: [0.69599409]\n",
            "y_pred: [0.5952274]|y_test: [0.60533423]\n",
            "y_pred: [0.5568596]|y_test: [0.46610205]\n",
            "y_pred: [0.5733794]|y_test: [0.36758053]\n",
            "y_pred: [0.5257149]|y_test: [0.6261693]\n",
            "y_pred: [0.52080345]|y_test: [0.41651457]\n",
            "y_pred: [0.5091544]|y_test: [0.52326547]\n",
            "y_pred: [0.48729342]|y_test: [0.51913501]\n",
            "y_pred: [0.45403236]|y_test: [0.58509699]\n",
            "y_pred: [0.48248872]|y_test: [0.41203001]\n",
            "y_pred: [0.45466256]|y_test: [0.74217919]\n",
            "y_pred: [0.44776285]|y_test: [0.58911503]\n",
            "y_pred: [0.5122284]|y_test: [0.51920347]\n",
            "y_pred: [0.5100731]|y_test: [0.39582924]\n",
            "y_pred: [0.47987625]|y_test: [0.45310578]\n",
            "y_pred: [0.48420373]|y_test: [0.61413117]\n",
            "y_pred: [0.4833833]|y_test: [0.68418563]\n",
            "y_pred: [0.48256448]|y_test: [0.78662425]\n",
            "y_pred: [0.5320642]|y_test: [0.59707166]\n",
            "y_pred: [0.52759916]|y_test: [0.58065705]\n",
            "y_pred: [0.5216343]|y_test: [0.57245761]\n",
            "y_pred: [0.568933]|y_test: [0.65817209]\n",
            "y_pred: [0.5134398]|y_test: [0.62123752]\n",
            "y_pred: [0.49958628]|y_test: [0.55605906]\n",
            "y_pred: [0.43217823]|y_test: [0.40567081]\n",
            "y_pred: [0.39915183]|y_test: [0.44602162]\n",
            "y_pred: [0.42367172]|y_test: [0.41721547]\n",
            "y_pred: [0.4239327]|y_test: [0.35131943]\n",
            "y_pred: [0.4392831]|y_test: [0.62613987]\n",
            "y_pred: [0.42400277]|y_test: [0.95451542]\n",
            "y_pred: [0.46192244]|y_test: [0.58049435]\n",
            "y_pred: [0.46023765]|y_test: [0.86108642]\n",
            "y_pred: [0.45618233]|y_test: [0.51962765]\n",
            "y_pred: [0.3736072]|y_test: [0.43878966]\n",
            "y_pred: [0.36004004]|y_test: [0.57626652]\n",
            "y_pred: [0.34684798]|y_test: [0.58433903]\n",
            "y_pred: [0.3388892]|y_test: [0.41849529]\n",
            "y_pred: [0.3295637]|y_test: [0.34921533]\n",
            "y_pred: [0.31913257]|y_test: [0.51128347]\n",
            "y_pred: [0.28772065]|y_test: [0.52348616]\n",
            "y_pred: [0.30807492]|y_test: [0.4297047]\n",
            "0.16499079450979778\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nklQIC12SCmGhS1IhLHRJKsT/A6+mZyxp/gdIAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_train_8\n",
            "\n",
            "\n",
            "y_pred: [0.52469045]|y_test: [0.82453399]\n",
            "y_pred: [0.5259057]|y_test: [0.51924743]\n",
            "y_pred: [0.52679694]|y_test: [0.30989464]\n",
            "y_pred: [0.5244654]|y_test: [0.72539204]\n",
            "y_pred: [0.5247739]|y_test: [0.48632996]\n",
            "y_pred: [0.53191215]|y_test: [0.61381076]\n",
            "y_pred: [0.5297095]|y_test: [0.50688718]\n",
            "y_pred: [0.5238671]|y_test: [0.69599409]\n",
            "y_pred: [0.521062]|y_test: [0.60533423]\n",
            "y_pred: [0.52484506]|y_test: [0.46610205]\n",
            "y_pred: [0.52321064]|y_test: [0.36758053]\n",
            "y_pred: [0.52792984]|y_test: [0.6261693]\n",
            "y_pred: [0.528407]|y_test: [0.41651457]\n",
            "y_pred: [0.5295592]|y_test: [0.52326547]\n",
            "y_pred: [0.5317302]|y_test: [0.51913501]\n",
            "y_pred: [0.53504366]|y_test: [0.58509699]\n",
            "y_pred: [0.5322074]|y_test: [0.41203001]\n",
            "y_pred: [0.53498274]|y_test: [0.74217919]\n",
            "y_pred: [0.5356704]|y_test: [0.58911503]\n",
            "y_pred: [0.52925074]|y_test: [0.51920347]\n",
            "y_pred: [0.5294648]|y_test: [0.39582924]\n",
            "y_pred: [0.5324675]|y_test: [0.45310578]\n",
            "y_pred: [0.53203684]|y_test: [0.61413117]\n",
            "y_pred: [0.5321175]|y_test: [0.68418563]\n",
            "y_pred: [0.53220004]|y_test: [0.78662425]\n",
            "y_pred: [0.5272857]|y_test: [0.59707166]\n",
            "y_pred: [0.52772754]|y_test: [0.58065705]\n",
            "y_pred: [0.5283181]|y_test: [0.57245761]\n",
            "y_pred: [0.523662]|y_test: [0.65817209]\n",
            "y_pred: [0.5291418]|y_test: [0.62123752]\n",
            "y_pred: [0.53051573]|y_test: [0.55605906]\n",
            "y_pred: [0.5372347]|y_test: [0.40567081]\n",
            "y_pred: [0.54056114]|y_test: [0.44602162]\n",
            "y_pred: [0.5380998]|y_test: [0.41721547]\n",
            "y_pred: [0.53808624]|y_test: [0.35131943]\n",
            "y_pred: [0.5365369]|y_test: [0.62613987]\n",
            "y_pred: [0.53808707]|y_test: [0.95451542]\n",
            "y_pred: [0.53431976]|y_test: [0.58049435]\n",
            "y_pred: [0.5344994]|y_test: [0.86108642]\n",
            "y_pred: [0.53488034]|y_test: [0.51962765]\n",
            "y_pred: [0.5432194]|y_test: [0.43878966]\n",
            "y_pred: [0.5445955]|y_test: [0.57626652]\n",
            "y_pred: [0.5460208]|y_test: [0.58433903]\n",
            "y_pred: [0.54681945]|y_test: [0.41849529]\n",
            "y_pred: [0.5478098]|y_test: [0.34921533]\n",
            "y_pred: [0.5489332]|y_test: [0.51128347]\n",
            "y_pred: [0.55211824]|y_test: [0.52348616]\n",
            "y_pred: [0.55004996]|y_test: [0.4297047]\n",
            "0.14020007141642712\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAD8RJREFUeJzt3X+M5PVdx/Hnu3cicEsPkGaKB+le\nEyRBrtLeWH80KbsexitU2ihJQSGcYlattUTPNNegaaJppBoaSSRW0pKjatimJ03J0dICZcQmgO4h\nshy0/Kho73ocrZVrl57STd/+sYMu192bH9/5zux8+nwkm5v5zmc+39d3bva13/3uzHciM5Ekjb9X\njTqAJGkwLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSIdYPc2VnnHFGTk5O1jL3\niy++yIYNG2qZexjMP1rjnh/GfxvMv7p9+/Z9IzNf02ncUAt9cnKSubm5WuZutVpMTU3VMvcwmH+0\nxj0/jP82mH91EfHv3YzzkIskFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBVi\nqO8UlTqZ3HXnSNa7e/v4vuVcepl76JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RC\nWOiSVAgLXZIKYaFLUiE6FnpE3BIRz0fEY8uW/XlEfCkiHo2IT0XEqfXGlCR10s0e+m5g+zHL7gbO\nz8w3AE8C7x9wLklSjzoWembeD3zzmGWfz8zF9tUHgbNqyCZJ6sEgjqH/OvDZAcwjSaogMrPzoIhJ\nYG9mnn/M8uuAJvBLucpEETEDzAA0Go2ts7OzFSOvbGFhgYmJiVrmHgbzL5k/eGQAaXq3eeO6sX78\nwefQqNWZf3p6el9mNjuN67vQI2IH8JvAtsz8Tjehms1mzs3NdTO0Z61Wi6mpqVrmHgbzLxnlB1yM\n8+MPPodGrc78EdFVoff1iUURsR14H3Bht2UuSapXNy9bvA14ADg3Ig5ExDXAXwKnAHdHxCMR8ZGa\nc0qSOui4h56ZV6yw+GM1ZJEkVeA7RSWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmF\nsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgL\nXZIKYaFLUiEsdEkqRMdCj4hbIuL5iHhs2bLTI+LuiHiq/e9p9caUJHXSzR76bmD7Mct2Afdm5jnA\nve3rkqQR6ljomXk/8M1jFr8DuLV9+VbgnQPOJUnqUb/H0BuZeah9+TmgMaA8kqQ+RWZ2HhQxCezN\nzPPb11/IzFOX3f5fmbnicfSImAFmABqNxtbZ2dkBxP5+CwsLTExM1DL3MJh/yfzBIwNI07vNG9eN\n9eMPPodGrc7809PT+zKz2Wnc+j7nPxwRZ2bmoYg4E3h+tYGZeTNwM0Cz2cypqak+V3l8rVaLuuYe\nBvMv2bHrzuph+rB7+4axfvzB59CorYX8/R5yuQO4un35auDTg4kjSepXNy9bvA14ADg3Ig5ExDXA\n9cDPR8RTwEXt65KkEep4yCUzr1jlpm0DziJJqsB3ikpSISx0SSqEhS5JhbDQJakQFrokFcJCl6RC\nWOiSVAgLXZIKYaFLUiEsdEkqRL9nW/yBMjmEMwDu3LL4fWcafPb6S2pfr6RyuIcuSYWw0CWpEBa6\nJBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJUKvSI+L2I2B8Rj0XE\nbRFx4qCCSZJ603ehR8Qm4L1AMzPPB9YBlw8qmCSpN1UPuawHToqI9cDJwNeqR5Ik9SMys/87R1wL\nfBA4Cnw+M391hTEzwAxAo9HYOjs72/f6jmdhYYGJiYla5p4/eKSWeZdrnASHj75y2ZZNG2tf70r6\n2d6V8o+TzRvX1fb8GZY6vweGwfyrm56e3peZzU7j+i70iDgN+HvgXcALwCeBPZn5t6vdp9ls5tzc\nXF/r66TVajE1NVXL3MP6gIsb5l/5eSOj+oCLfrZ3pfzjZPf2DbU9f4alzu+BYTD/6iKiq0Kvcsjl\nIuDfMvPrmfld4HbgZyvMJ0mqoEqh/wfw0xFxckQEsA14YjCxJEm96rvQM/MhYA/wMDDfnuvmAeWS\nJPWo0kHPzPwA8IEBZZEkVeA7RSWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAl\nqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIK\nYaFLUiEsdEkqRKVCj4hTI2JPRHwpIp6IiJ8ZVDBJUm/WV7z/jcBdmXlZRJwAnDyATJKkPvRd6BGx\nEXgrsAMgM18CXhpMLElSryIz+7tjxAXAzcDjwE8A+4BrM/PFY8bNADMAjUZj6+zsbKXAq1lYWGBi\nYqKWuecPHqll3uUaJ8Hho69ctmXTxtrXu5J+tnel/ONk88Z1tT1/hqXO74FhMP/qpqen92Vms9O4\nKoXeBB4E3pKZD0XEjcC3MvOPVrtPs9nMubm5vtbXSavVYmpqqpa5J3fdWcu8y+3cssgN86/8henZ\n6y+pfb0r6Wd7V8o/TnZv31Db82dY6vweGAbzry4iuir0Kn8UPQAcyMyH2tf3AG+qMJ8kqYK+Cz0z\nnwO+GhHnthdtY+nwiyRpBKr+jvy7wN+1X+HyFeDXqkeSJPWjUqFn5iNAx+M6kqT6+U5RSSqEhS5J\nhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYUY3/Od/gAYxml7JZXDPXRJKoSF\nLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQlQs9ItZFxL9ExN5B\nBJIk9WcQe+jXAk8MYB5JUgWVCj0izgIuAT46mDiSpH5V3UP/C+B9wPcGkEWSVEFkZn93jHg7cHFm\nvjsipoA/yMy3rzBuBpgBaDQaW2dnZ/ta3/zBI8e9vXESHD7a19RrgvlHa/PGdUxMTIw6RiULCwtj\nvQ3mX9309PS+zGx2Glel0P8UuApYBE4EXg3cnplXrnafZrOZc3Nzfa2v04c97NyyyA3z4/t5HeYf\nrd3bNzA1NTXqGJW0Wq2x3gbzry4iuir0vg+5ZOb7M/OszJwELge+cLwylyTVy9ehS1IhBvI7cma2\ngNYg5pIk9cc9dEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAl\nqRDje75TqRCdTg3drZ1bFtnRw1zPXn/JQNartcM9dEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQI\nC12SCmGhS1IhLHRJKoSFLkmFsNAlqRB9F3pEnB0R90XE4xGxPyKuHWQwSVJvqpycaxHYmZkPR8Qp\nwL6IuDszHx9QNklSD/reQ8/MQ5n5cPvyt4EngE2DCiZJ6s1AjqFHxCTwRuChQcwnSepdZGa1CSIm\ngH8APpiZt69w+wwwA9BoNLbOzs72tZ75g0eOe3vjJDh8tK+p1wTzj9a454fet2HLpo31helgpe/n\nYfwf1LnNCwsLTExM1DL39PT0vsxsdhpXqdAj4oeAvcDnMvPDncY3m82cm5vra12dPgRg55ZFbpgf\n38/rMP9ojXt+6H0bRvkBFyt9Pw/j/6DObW61WkxNTdUyd0R0VehVXuUSwMeAJ7opc0lSvaocQ38L\ncBXwcxHxSPvr4gHlkiT1qO/fbzLzi0AMMIskqQLfKSpJhbDQJakQFrokFcJCl6RCWOiSVAgLXZIK\nYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBVivM8XKqlvnU5JXaI6t3nnlkV2HGf+YZyu2D10SSqEhS5J\nhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEJUKPSK2R8SXI+LpiNg1\nqFCSpN71XegRsQ64CXgbcB5wRUScN6hgkqTeVNlDfzPwdGZ+JTNfAmaBdwwmliSpV1UKfRPw1WXX\nD7SXSZJGIDKzvztGXAZsz8zfaF+/CvipzHzPMeNmgJn21XOBL/cf97jOAL5R09zDYP7RGvf8MP7b\nYP7VvS4zX9NpUJUPuDgInL3s+lntZa+QmTcDN1dYT1ciYi4zm3Wvpy7mH61xzw/jvw3mr67KIZd/\nBs6JiM0RcQJwOXDHYGJJknrV9x56Zi5GxHuAzwHrgFsyc//AkkmSelLpM0Uz8zPAZwaUparaD+vU\nzPyjNe75Yfy3wfwV9f1HUUnS2uJb/yWpEGNb6BFxekTcHRFPtf89bYUxF0TEAxGxPyIejYh3jSLr\nSrrJ3x53V0S8EBF7h51xJZ1O9xARPxwRn2jf/lBETA4/5eq6yP/WiHg4IhbbL81dU7rI//sR8Xj7\n+X5vRLxuFDlX00X+34qI+Yh4JCK+uNbefd7t6U4i4pcjIiNiuK96ycyx/AL+DNjVvrwL+NAKY34M\nOKd9+UeBQ8Cpo87ebf72bduAXwT2roHM64BngNcDJwD/Cpx3zJh3Ax9pX74c+MSoc/eYfxJ4A/Bx\n4LJRZ+4j/zRwcvvyb4/h4//qZZcvBe4ade5e8rfHnQLcDzwINIeZcWz30Fk6zcCt7cu3Au88dkBm\nPpmZT7Uvfw14Huj44vwh6ZgfIDPvBb49rFAddHO6h+XbtQfYFhExxIzH0zF/Zj6bmY8C3xtFwA66\nyX9fZn6nffVBlt4fslZ0k/9by65uANbSH/m6Pd3JnwAfAv57mOFgjA+5AI3MPNS+/BzQON7giHgz\nSz9Vn6k7WJd6yr9GdHO6h/8bk5mLwBHgR4aSrrNxP11Fr/mvAT5ba6LedJU/In4nIp5h6bfY9w4p\nWzc65o+INwFnZ+adwwz2skovW6xbRNwDvHaFm65bfiUzMyJW/UkeEWcCfwNcnZlD2/MaVH6pVxFx\nJdAELhx1ll5l5k3ATRHxK8AfAlePOFJXIuJVwIeBHaPKsKYLPTMvWu22iDgcEWdm5qF2YT+/yrhX\nA3cC12XmgzVFXdEg8q8x3Zzu4eUxByJiPbAR+M/hxOuoq9NVrGFd5Y+Ii1jaabgwM/9nSNm60evj\nPwv8Va2JetMp/ynA+UCrfZTxtcAdEXFpZs4NI+A4H3K5g///yX018OljB7RPSfAp4OOZuWeI2brR\nMf8a1M3pHpZv12XAF7L9l6I1YNxPV9Exf0S8Efhr4NLMXGs7Cd3kP2fZ1UuAp4aYr5Pj5s/MI5l5\nRmZOZuYkS3/DGFqZvxxiLL9YOi57L0v/4fcAp7eXN4GPti9fCXwXeGTZ1wWjzt5t/vb1fwS+Dhxl\n6ZjdL4w498XAkyz9LeK69rI/ZumJC3Ai8EngaeCfgNeP+rHuMf9Pth/nF1n6zWL/qDP3mP8e4PCy\n5/sdo87cY/4bgf3t7PcBPz7qzL3kP2ZsiyG/ysV3ikpSIcb5kIskaRkLXZIKYaFLUiEsdEkqhIUu\nSYWw0CWpEBa6JBXCQpekQvwveC3OC8k0jE0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_train_9\n",
            "\n",
            "\n",
            "y_pred: [0.54530466]|y_test: [0.82453399]\n",
            "y_pred: [0.5388318]|y_test: [0.51924743]\n",
            "y_pred: [0.53511715]|y_test: [0.30989464]\n",
            "y_pred: [0.5454286]|y_test: [0.72539204]\n",
            "y_pred: [0.54357594]|y_test: [0.48632996]\n",
            "y_pred: [0.514999]|y_test: [0.61381076]\n",
            "y_pred: [0.52324617]|y_test: [0.50688718]\n",
            "y_pred: [0.54619837]|y_test: [0.69599409]\n",
            "y_pred: [0.55750453]|y_test: [0.60533423]\n",
            "y_pred: [0.5406626]|y_test: [0.46610205]\n",
            "y_pred: [0.54644984]|y_test: [0.36758053]\n",
            "y_pred: [0.5277982]|y_test: [0.6261693]\n",
            "y_pred: [0.52333456]|y_test: [0.41651457]\n",
            "y_pred: [0.5172347]|y_test: [0.52326547]\n",
            "y_pred: [0.5074598]|y_test: [0.51913501]\n",
            "y_pred: [0.4927982]|y_test: [0.58509699]\n",
            "y_pred: [0.5050512]|y_test: [0.41203001]\n",
            "y_pred: [0.49410307]|y_test: [0.74217919]\n",
            "y_pred: [0.4905526]|y_test: [0.58911503]\n",
            "y_pred: [0.5164579]|y_test: [0.51920347]\n",
            "y_pred: [0.51582754]|y_test: [0.39582924]\n",
            "y_pred: [0.50386596]|y_test: [0.45310578]\n",
            "y_pred: [0.50576866]|y_test: [0.61413117]\n",
            "y_pred: [0.50483054]|y_test: [0.68418563]\n",
            "y_pred: [0.5051376]|y_test: [0.78662425]\n",
            "y_pred: [0.52460515]|y_test: [0.59707166]\n",
            "y_pred: [0.522534]|y_test: [0.58065705]\n",
            "y_pred: [0.51994365]|y_test: [0.57245761]\n",
            "y_pred: [0.5359894]|y_test: [0.65817209]\n",
            "y_pred: [0.51041216]|y_test: [0.62123752]\n",
            "y_pred: [0.50431645]|y_test: [0.55605906]\n",
            "y_pred: [0.47725075]|y_test: [0.40567081]\n",
            "y_pred: [0.46140826]|y_test: [0.44602162]\n",
            "y_pred: [0.4707507]|y_test: [0.41721547]\n",
            "y_pred: [0.468625]|y_test: [0.35131943]\n",
            "y_pred: [0.47675085]|y_test: [0.62613987]\n",
            "y_pred: [0.46747118]|y_test: [0.95451542]\n",
            "y_pred: [0.4797625]|y_test: [0.58049435]\n",
            "y_pred: [0.4778477]|y_test: [0.86108642]\n",
            "y_pred: [0.47875088]|y_test: [0.51962765]\n",
            "y_pred: [0.44137537]|y_test: [0.43878966]\n",
            "y_pred: [0.4359408]|y_test: [0.57626652]\n",
            "y_pred: [0.424254]|y_test: [0.58433903]\n",
            "y_pred: [0.4220329]|y_test: [0.41849529]\n",
            "y_pred: [0.41587776]|y_test: [0.34921533]\n",
            "y_pred: [0.40848547]|y_test: [0.51128347]\n",
            "y_pred: [0.39886355]|y_test: [0.52348616]\n",
            "y_pred: [0.40526474]|y_test: [0.4297047]\n",
            "0.14753736896969719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_train_10\n",
            "\n",
            "\n",
            "y_pred: [0.5826648]|y_test: [0.82453399]\n",
            "y_pred: [0.5807064]|y_test: [0.51924743]\n",
            "y_pred: [0.5795728]|y_test: [0.30989464]\n",
            "y_pred: [0.5827061]|y_test: [0.72539204]\n",
            "y_pred: [0.5821479]|y_test: [0.48632996]\n",
            "y_pred: [0.57335263]|y_test: [0.61381076]\n",
            "y_pred: [0.57592195]|y_test: [0.50688718]\n",
            "y_pred: [0.5829472]|y_test: [0.69599409]\n",
            "y_pred: [0.58634037]|y_test: [0.60533423]\n",
            "y_pred: [0.5812801]|y_test: [0.46610205]\n",
            "y_pred: [0.5830347]|y_test: [0.36758053]\n",
            "y_pred: [0.5773553]|y_test: [0.6261693]\n",
            "y_pred: [0.57601196]|y_test: [0.41651457]\n",
            "y_pred: [0.5741507]|y_test: [0.52326547]\n",
            "y_pred: [0.5711386]|y_test: [0.51913501]\n",
            "y_pred: [0.56658953]|y_test: [0.58509699]\n",
            "y_pred: [0.5703981]|y_test: [0.41203001]\n",
            "y_pred: [0.5669766]|y_test: [0.74217919]\n",
            "y_pred: [0.56588024]|y_test: [0.58911503]\n",
            "y_pred: [0.5739387]|y_test: [0.51920347]\n",
            "y_pred: [0.57374096]|y_test: [0.39582924]\n",
            "y_pred: [0.5700316]|y_test: [0.45310578]\n",
            "y_pred: [0.5706206]|y_test: [0.61413117]\n",
            "y_pred: [0.57033926]|y_test: [0.68418563]\n",
            "y_pred: [0.5704239]|y_test: [0.78662425]\n",
            "y_pred: [0.57644016]|y_test: [0.59707166]\n",
            "y_pred: [0.5758087]|y_test: [0.58065705]\n",
            "y_pred: [0.5750158]|y_test: [0.57245761]\n",
            "y_pred: [0.5799392]|y_test: [0.65817209]\n",
            "y_pred: [0.5721699]|y_test: [0.62123752]\n",
            "y_pred: [0.57029647]|y_test: [0.55605906]\n",
            "y_pred: [0.561862]|y_test: [0.40567081]\n",
            "y_pred: [0.5569449]|y_test: [0.44602162]\n",
            "y_pred: [0.5598927]|y_test: [0.41721547]\n",
            "y_pred: [0.55927706]|y_test: [0.35131943]\n",
            "y_pred: [0.5617791]|y_test: [0.62613987]\n",
            "y_pred: [0.55894226]|y_test: [0.95451542]\n",
            "y_pred: [0.5628476]|y_test: [0.58049435]\n",
            "y_pred: [0.56227684]|y_test: [0.86108642]\n",
            "y_pred: [0.5625054]|y_test: [0.51962765]\n",
            "y_pred: [0.55089134]|y_test: [0.43878966]\n",
            "y_pred: [0.5491835]|y_test: [0.57626652]\n",
            "y_pred: [0.54567766]|y_test: [0.58433903]\n",
            "y_pred: [0.54495674]|y_test: [0.41849529]\n",
            "y_pred: [0.54308903]|y_test: [0.34921533]\n",
            "y_pred: [0.5408555]|y_test: [0.51128347]\n",
            "y_pred: [0.53777605]|y_test: [0.52348616]\n",
            "y_pred: [0.5398192]|y_test: [0.4297047]\n",
            "0.1359067443583101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDxv_Qv6hS-A",
        "colab_type": "text"
      },
      "source": [
        "6. Creating Ensemble  2-layer sigmoid-activated ANN (i.e. a combination of all 10 models trained above) and test it with the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsgV8HI6Ewcv",
        "colab_type": "code",
        "outputId": "9606b550-9ed4-4286-f7b1-b2c36d1f2816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "y_pred_1= model_train_1.predict(X_test)\n",
        "y_pred_2= model_train_2.predict(X_test)\n",
        "y_pred_3= model_train_3.predict(X_test)\n",
        "y_pred_4= model_train_4.predict(X_test)\n",
        "y_pred_5= model_train_5.predict(X_test)\n",
        "y_pred_6= model_train_6.predict(X_test)\n",
        "y_pred_7= model_train_7.predict(X_test)\n",
        "y_pred_8= model_train_8.predict(X_test)\n",
        "y_pred_9= model_train_9.predict(X_test)\n",
        "y_pred_10= model_train_10.predict(X_test)\n",
        "\n",
        "y_pred= (y_pred_1 + y_pred_2 + y_pred_3 + y_pred_4 + y_pred_5 + y_pred_6 + y_pred_7 + y_pred_8 + y_pred_9 + y_pred_10)/10\n",
        "\n",
        "y_diff=[]\n",
        "for j in range(len(y_pred)):\n",
        "  print('y_pred: ' + str(y_pred[j]) + '|' + 'y_test: ' + str(y_test[j]))\n",
        "  y_diff.append(float(y_test[j]- y_pred[j]))\n",
        "\n",
        "print(MSE(y_test,y_pred)**0.5)\n",
        "pd.Series(y_diff).hist()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_pred: [0.546714]|y_test: [0.82453399]\n",
            "y_pred: [0.5427681]|y_test: [0.51924743]\n",
            "y_pred: [0.5405981]|y_test: [0.30989464]\n",
            "y_pred: [0.54668677]|y_test: [0.72539204]\n",
            "y_pred: [0.5455385]|y_test: [0.48632996]\n",
            "y_pred: [0.5289458]|y_test: [0.61381076]\n",
            "y_pred: [0.5336786]|y_test: [0.50688718]\n",
            "y_pred: [0.54692566]|y_test: [0.69599409]\n",
            "y_pred: [0.5534501]|y_test: [0.60533423]\n",
            "y_pred: [0.5435225]|y_test: [0.46610205]\n",
            "y_pred: [0.5467725]|y_test: [0.36758053]\n",
            "y_pred: [0.5359999]|y_test: [0.6261693]\n",
            "y_pred: [0.5330845]|y_test: [0.41651457]\n",
            "y_pred: [0.5293466]|y_test: [0.52326547]\n",
            "y_pred: [0.5235058]|y_test: [0.51913501]\n",
            "y_pred: [0.51474416]|y_test: [0.58509699]\n",
            "y_pred: [0.5220353]|y_test: [0.41203001]\n",
            "y_pred: [0.51564115]|y_test: [0.74217919]\n",
            "y_pred: [0.51345813]|y_test: [0.58911503]\n",
            "y_pred: [0.52864385]|y_test: [0.51920347]\n",
            "y_pred: [0.5283043]|y_test: [0.39582924]\n",
            "y_pred: [0.5213239]|y_test: [0.45310578]\n",
            "y_pred: [0.5224593]|y_test: [0.61413117]\n",
            "y_pred: [0.52183414]|y_test: [0.68418563]\n",
            "y_pred: [0.52209294]|y_test: [0.78662425]\n",
            "y_pred: [0.5334233]|y_test: [0.59707166]\n",
            "y_pred: [0.5321807]|y_test: [0.58065705]\n",
            "y_pred: [0.53064454]|y_test: [0.57245761]\n",
            "y_pred: [0.53965276]|y_test: [0.65817209]\n",
            "y_pred: [0.5243185]|y_test: [0.62123752]\n",
            "y_pred: [0.52068156]|y_test: [0.55605906]\n",
            "y_pred: [0.5047707]|y_test: [0.40567081]\n",
            "y_pred: [0.49511737]|y_test: [0.44602162]\n",
            "y_pred: [0.50057065]|y_test: [0.41721547]\n",
            "y_pred: [0.49905285]|y_test: [0.35131943]\n",
            "y_pred: [0.50407577]|y_test: [0.62613987]\n",
            "y_pred: [0.49823317]|y_test: [0.95451542]\n",
            "y_pred: [0.50514066]|y_test: [0.58049435]\n",
            "y_pred: [0.50386983]|y_test: [0.86108642]\n",
            "y_pred: [0.5046932]|y_test: [0.51962765]\n",
            "y_pred: [0.48213416]|y_test: [0.43878966]\n",
            "y_pred: [0.47892132]|y_test: [0.57626652]\n",
            "y_pred: [0.47127485]|y_test: [0.58433903]\n",
            "y_pred: [0.47007203]|y_test: [0.41849529]\n",
            "y_pred: [0.46615583]|y_test: [0.34921533]\n",
            "y_pred: [0.46141952]|y_test: [0.51128347]\n",
            "y_pred: [0.45606622]|y_test: [0.52348616]\n",
            "y_pred: [0.4596528]|y_test: [0.4297047]\n",
            "0.13987237955827048\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJakQ/wOaE91CsCRXSwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1L2X5Hnlte9",
        "colab_type": "text"
      },
      "source": [
        "7. Save all the trained models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWF54RKAXtAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_train_1.save('EURUSD_model1.h5')\n",
        "model_train_2.save('EURUSD_model2.h5')\n",
        "model_train_3.save('EURUSD_model3.h5')\n",
        "model_train_4.save('EURUSD_model4.h5')\n",
        "model_train_5.save('EURUSD_model5.h5')\n",
        "model_train_6.save('EURUSD_model6.h5')\n",
        "model_train_7.save('EURUSD_model7.h5')\n",
        "model_train_8.save('EURUSD_model8.h5')\n",
        "model_train_9.save('EURUSD_model9.h5')\n",
        "model_train_10.save('EURUSD_model10.h5')\n",
        "\n",
        "\n",
        "!cp EURUSD_model1.h5 drive/My\\ Drive/Trading\\ EURUSD\\ with\\ Deep\\ Learning\\ Algorithms/\n",
        "!cp EURUSD_model2.h5 drive/My\\ Drive/Trading\\ EURUSD\\ with\\ Deep\\ Learning\\ Algorithms/\n",
        "!cp EURUSD_model3.h5 drive/My\\ Drive/Trading\\ EURUSD\\ with\\ Deep\\ Learning\\ Algorithms/\n",
        "!cp EURUSD_model4.h5 drive/My\\ Drive/Trading\\ EURUSD\\ with\\ Deep\\ Learning\\ Algorithms/\n",
        "!cp EURUSD_model5.h5 drive/My\\ Drive/Trading\\ EURUSD\\ with\\ Deep\\ Learning\\ Algorithms/\n",
        "!cp EURUSD_model6.h5 drive/My\\ Drive/Trading\\ EURUSD\\ with\\ Deep\\ Learning\\ Algorithms/\n",
        "!cp EURUSD_model7.h5 drive/My\\ Drive/Trading\\ EURUSD\\ with\\ Deep\\ Learning\\ Algorithms/\n",
        "!cp EURUSD_model8.h5 drive/My\\ Drive/Trading\\ EURUSD\\ with\\ Deep\\ Learning\\ Algorithms/\n",
        "!cp EURUSD_model9.h5 drive/My\\ Drive/Trading\\ EURUSD\\ with\\ Deep\\ Learning\\ Algorithms/\n",
        "!cp EURUSD_model10.h5 drive/My\\ Drive/Trading\\ EURUSD\\ with\\ Deep\\ Learning\\ Algorithms/\n",
        "\n",
        "X_train_min_c1= X_train_pre[:,0].min()\n",
        "X_train_max_c1= X_train_pre[:,0].max()\n",
        "X_train_min_c2= X_train_pre[:,1].min()\n",
        "X_train_max_c2= X_train_pre[:,1].max()\n",
        "y_train_min= y_train_pre.reshape(-1,1).min()\n",
        "y_train_max= y_train_pre.reshape(-1,1).max()\n",
        "\n",
        "print('X_train_min_c1 :' + str(X_train_min_c1))\n",
        "print('X_train_max_c1 :' + str(X_train_max_c1))\n",
        "print('X_train_min_c2 :' + str(X_train_min_c2))\n",
        "print('X_train_max_c2 :' + str(X_train_max_c2))\n",
        "print('y_train_min :' + str(y_train_min))\n",
        "print('y_train_max :' + str(y_train_max))\n",
        "\n",
        "scaled_params= pd.DataFrame({'y_train_mean':[mean_return],\n",
        "                             'y_train_sd':[sd_return],\n",
        "                             'X_train_min_c1':[X_train_min_c1],\n",
        "                             'X_train_max_c1':[X_train_max_c1],\n",
        "                             'X_train_min_c2':[X_train_min_c2],\n",
        "                             'X_train_max_c2':[X_train_max_c2],\n",
        "                             'y_train_min':[y_train_min],\n",
        "                             'y_train_max':[y_train_max]})\n",
        "\n",
        "scaled_params.to_csv('scaled_params.csv',index=False)\n",
        "!cp scaled_params.csv drive/My\\ Drive/Trading\\ EURUSD\\ with\\ Deep\\ Learning\\ Algorithms/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEQPrtw6lmKm",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}